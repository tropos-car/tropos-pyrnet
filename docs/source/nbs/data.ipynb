{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data level\n",
    "\n",
    "Reading and advance in data processing levels to be stored in netcdf database format.\n",
    "\n",
    "```{note}\n",
    "raw > l1a > l1b > l1c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import logging\n",
    "from toolz import assoc_in\n",
    "import pkg_resources as pkg_res\n",
    "import warnings\n",
    "\n",
    "from trosat import sunpos as sp\n",
    "\n",
    "import pyrnet as pyrnet_main\n",
    "pyrnet_version = pyrnet_main.__version__\n",
    "from pyrnet import pyrnet\n",
    "from pyrnet import utils as pyrutils\n",
    "from pyrnet import logger as pyrlogger\n",
    "from pyrnet import reports as pyrreports\n",
    "\n",
    "# logging setup\n",
    "logging.basicConfig(\n",
    "    filename='pyrnet.log',\n",
    "    encoding='utf-8',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(name)s %(levelname)s:%(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## netCDF dataset operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T15:31:29.503812900Z",
     "start_time": "2024-01-31T15:31:27.789566200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def update_coverage_meta(ds,timevar='time'):\n",
    "    \"\"\"Update global attributes related to geospatial and time coverage\n",
    "    \"\"\"\n",
    "    duration = ds[timevar].values[-1] - ds[timevar].values[0]\n",
    "    resolution = np.mean(np.diff(ds[timevar].values))\n",
    "    now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "    gattrs = {\n",
    "        'date_created': now.isoformat(),\n",
    "        'geospatial_lat_min': np.nanmin(ds.lat.values),\n",
    "        'geospatial_lat_max': np.nanmax(ds.lat.values),\n",
    "        'geospatial_lat_units': 'degN',\n",
    "        'geospatial_lon_min': np.nanmin(ds.lon.values),\n",
    "        'geospatial_lon_max': np.nanmax(ds.lon.values),\n",
    "        'geospatial_lon_units': 'degE',\n",
    "        'time_coverage_start': pd.to_datetime(ds[timevar].values[0]).isoformat(),\n",
    "        'time_coverage_end': pd.to_datetime(ds[timevar].values[-1]).isoformat(),\n",
    "        'time_coverage_duration': pd.to_timedelta(duration).isoformat(),\n",
    "        'time_coverage_resolution': pd.to_timedelta(resolution).isoformat(),\n",
    "    }\n",
    "    ds.attrs.update(gattrs)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T15:31:29.517377200Z",
     "start_time": "2024-01-31T15:31:27.789566200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#|export\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstretch_resolution\u001b[39m(ds: xr\u001b[38;5;241m.\u001b[39mDataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\" Stretch variable resolution to full integer size,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    to not lose resolution after averaging ADC count data.\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m ds:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "def stretch_resolution(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\" Stretch variable resolution to full integer size,\n",
    "    to not lose resolution after averaging ADC count data.\"\"\"\n",
    "    for var in ds:\n",
    "        if \"scale_factor\" not in ds[var].encoding:\n",
    "            continue\n",
    "        if \"valid_range\" not in ds[var].attrs:\n",
    "            continue\n",
    "        dtype = ds[var].encoding['dtype']\n",
    "        valid_range = ds[var].valid_range\n",
    "        int_limit = np.iinfo(dtype).max\n",
    "        scale_factor = ds[var].encoding['scale_factor']\n",
    "        scale_factor_mod = int((int_limit-1)/valid_range[1])\n",
    "        ds[var].encoding.update({\n",
    "            \"scale_factor\": scale_factor / scale_factor_mod,\n",
    "            \"_FillValue\": int_limit,\n",
    "        })\n",
    "        ds[var].attrs.update({\n",
    "            \"valid_range\": valid_range * scale_factor_mod\n",
    "        })\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.100719900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def merge_ds(ds1,ds2,timevar=\"time\"):\n",
    "    \"\"\"Merge two datasets along the time dimension.\n",
    "    \"\"\"\n",
    "    if ds1[timevar].equals(ds2[timevar]):\n",
    "        logging.info(\"Overwrite existing file.\")\n",
    "        return ds2\n",
    "    logging.info(\"Merge with existing file.\")\n",
    "\n",
    "    ## overwrite non time dependent variables\n",
    "    overwrite_vars = [ v for v in ds1 if timevar not in ds1[v].dims ]\n",
    "\n",
    "    ## merge both datasets\n",
    "    ds_new=ds1.merge(ds2,\n",
    "                     compat='no_conflicts',\n",
    "                     overwrite_vars=overwrite_vars)\n",
    "\n",
    "    # add global coverage attributes\n",
    "    ds_new.attrs.update({'merged':1})\n",
    "\n",
    "    # add encoding again\n",
    "    ds_new = add_encoding(ds_new)\n",
    "    return ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.100719900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_netcdf(ds,fname, timevar=\"time\"):\n",
    "    \"\"\"xarray to netcdf, but merge if exist\n",
    "    \"\"\"\n",
    "    # merge if necessary\n",
    "    if os.path.exists(fname):\n",
    "        ds1 = xr.open_dataset(fname)\n",
    "        ds = merge_ds(ds1,ds, timevar=timevar)\n",
    "        ds1.close()\n",
    "        os.remove(fname)\n",
    "\n",
    "    # save to netCDF4\n",
    "    ds = update_coverage_meta(ds, timevar=timevar)\n",
    "    ds.to_netcdf(fname,\n",
    "                 encoding={timevar:{'dtype':'float64'}}) # for OpenDAP 2 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.116231Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def resample(ds, freq, methods='mean', kwargs={}):\n",
    "    \"\"\" Resample xarray dataset using pandas for speed.\n",
    "    https://github.com/pydata/xarray/issues/4498#issuecomment-706688398\n",
    "    \"\"\"\n",
    "    if isinstance(methods,str):\n",
    "        methods = [methods]\n",
    "\n",
    "    dsr = ds.to_dataframe().resample(freq)\n",
    "    dsouts = []\n",
    "    for method in methods:\n",
    "        # what we want (quickly), but in Pandas form\n",
    "        df_h = dsr.apply(method)\n",
    "        # rebuild xarray dataset with attributes\n",
    "        vals = []\n",
    "        for c in df_h.columns:\n",
    "            vals.append(\n",
    "                xr.DataArray(data=df_h[c],\n",
    "                             dims=['time'],\n",
    "                             coords={'time': df_h.index},\n",
    "                             attrs=ds[c].attrs)\n",
    "            )\n",
    "        dsouts.append(xr.Dataset(dict(zip(df_h.columns, vals)), attrs=ds.attrs))\n",
    "\n",
    "    if len(dsouts) == 1:\n",
    "        dsouts = dsouts[0]\n",
    "    return dsouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Default config and nc encoding\n",
    "Parse default config and cfmeta, and merge with user condfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.131859300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_config(config: dict|None = None) -> dict:\n",
    "    \"\"\"Read default config and merge with input config\n",
    "    \"\"\"\n",
    "\n",
    "    fn_config = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_config.json\")\n",
    "    default_config = pyrutils.read_json(fn_config)\n",
    "    if config is None:\n",
    "        config = default_config\n",
    "    config = {**default_config, **config}\n",
    "\n",
    "    # add default files\n",
    "    cfiles = {\n",
    "        \"file_cfmeta\": \"share/pyrnet_cfmeta.json\",\n",
    "        \"file_calibration\": \"share/pyrnet_calibration.json\",\n",
    "        \"file_mapping\": \"share/pyrnet_station_map.json\",\n",
    "        \"file_gti_angles\": \"share/pyrnet_gti_angles.json\",\n",
    "        \"file_site\": \"share/pyrnet_sites.json\",\n",
    "    }\n",
    "    for fn in cfiles:\n",
    "        if config[fn] is None:\n",
    "            config[fn] =  pkg_res.resource_filename(\"pyrnet\", cfiles[fn])\n",
    "    return config\n",
    "\n",
    "def get_sensor_config(sconfig: dict|None = None) -> dict:\n",
    "    \"\"\" Read the sensor configuration from the default json file and merge if needed. \n",
    "    \"\"\"\n",
    "    fn_config = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_sensor_config.json\")\n",
    "    default_config = pyrutils.read_json(fn_config)\n",
    "    if sconfig is None:\n",
    "        sconfig = default_config\n",
    "    sconfig = {**default_config, **sconfig}\n",
    "    return sconfig\n",
    "    \n",
    "def get_cfmeta(config: dict|None = None) -> dict:\n",
    "    \"\"\"Read global and variable attributes and encoding from cfmeta.json\n",
    "    \"\"\"\n",
    "    config= get_config(config)\n",
    "    # parse the json file\n",
    "    cfdict = pyrutils.read_json(config[\"file_cfmeta\"])\n",
    "    # get global attributes:\n",
    "    gattrs = cfdict['attributes']\n",
    "    # apply config\n",
    "    gattrs = {k:v.format_map(config) for k,v in gattrs.items()}\n",
    "    # get variable attributes\n",
    "    d = pyrutils.get_var_attrs(cfdict)\n",
    "    # split encoding attributes\n",
    "    vattrs, vencode = pyrutils.get_attrs_enc(d)\n",
    "    return gattrs ,vattrs, vencode\n",
    "\n",
    "def calc_encoding(sconfig:dict, ADCV=3.3, ADCbits=10) -> dict:\n",
    "    ADCfac = ADCV / (2**ADCbits-1) # Last bit is reserved \n",
    "    sencoding ={}\n",
    "    for k,v in sconfig.items():\n",
    "        sencoding.update(\n",
    "            {k: dict(\n",
    "                units=v['units'],\n",
    "                scale_factor=v['C']*ADCfac/v['gain'],\n",
    "                add_offset=v['offset'],\n",
    "                valid_range= [0, min(((2**ADCbits-1), int(v['Vmax']*v['gain']/ADCfac)))]\n",
    "            )}\n",
    "        )\n",
    "    return sencoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.138821800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sconfig = get_sensor_config()\n",
    "sencoding = calc_encoding(sconfig)\n",
    "sencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoding\n",
    "The encoding attributes are defined within the file PyrNet/share/pyrnet_cfmeta.json per default.\n",
    "\n",
    "The netcdf packing of data is realized via the two attributes *scale_factor* and *add_offset*:\n",
    "  ```unpacked_variable = scale_factor * packed_variable + add_offset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.149331600Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "fn_cfjson = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_cfmeta.json\")\n",
    "pyrutils.read_json(fn_cfjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Irradiance values\n",
    "\n",
    "Irradiance values are stored as Voltage for later calibration. Assigning 1500Wm-2 as maximum measurable irradiance from the irradiance sensor. The maximum counts (*valid_max*) measured by the logger can be calculated by:\n",
    " * maximum counts = $\\mathrm{gain}*1500*\\mathrm{C_{max}}*1023/3.3$\n",
    "\n",
    "Most calibration factors are in the area of 7.5 uV/Wm-2, assuming $\\mathrm{C_{max}}=8$uV/Wm-2 for this estimate seems sufficient. All radiation sensors are amplified with a fixed gain of $\\mathrm{gain}=300$.\n",
    "\n",
    "Later (level 1c+) the scale factor will be instrument specific by adding the calibration from V to W m-2\n",
    "\n",
    "### Temperature and humidity\n",
    "\n",
    "\n",
    "Calibration coefficients for Temperature and Humidity DKRF-4000 series (discontinued) https://www.driesen-kern.de/downloads/produktlinie_feuchte.pdf are:\n",
    " * Temperature (T) range :-20-80 degC  from 0-5V\n",
    " * relative Humidity (rH) range: 0-100% from 0-5V\n",
    "\n",
    "As the logger ADC range is 0-3.3V with a 10bit resolution, the sensors are measured through a voltage splitting circuit. Therefore, the ADC counts have to be doubled.\n",
    " * Voltage U [V] = $2* 3.3/1023$ [counts]\n",
    " * T = $-20 + 100*(U/5)$ [degC] = $253.15 + 100*(U/5)$ [K]\n",
    " * rH = $(U/5)$  [%]  = $(U/5)$    [%]\n",
    "\n",
    "### Ancillary data\n",
    "\n",
    "* solar zenith angle (*szen*)\n",
    "    * valid range unpacked: (0,180) (deg)\n",
    "    * packing in u2 integer (unsigned 16bit)\n",
    "        * fill value = $2^{16} - 1$\n",
    "        * scale_factor = $180 /(2^{16}-2)$\n",
    "* solar azimuth angle (*sazi*)\n",
    "    * valid range unpacked: (0,360)\n",
    "    * packing in u2 integer (unsigned 16bit)\n",
    "        * fill_value = $2^{16} -1$\n",
    "        * scale_factor= $360 / (2^{16}-2)$\n",
    "* earth sun distance (*esd*)\n",
    "    * valid range unpacked: (0.98,1.02)\n",
    "    * packing in u2 integer (unsigned 16bit)\n",
    "        * fill_value = $2^{16} -1$\n",
    "        * scale_factor= $(1.02-0.98)/(2^{16}-2)$\n",
    "        * add_offset = 0.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.157354600Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "sensor_config = {\n",
    "    \"ta\": {\n",
    "        \"C\": 100./5., # K/V\n",
    "        \"offset\": 253.15, # K at 0V\n",
    "        \"gain\": 0.5, # measured over voltage splitter\n",
    "        \"Vmax\": 5, # V output at sensor\n",
    "    },\n",
    "    \"rh\": {\n",
    "        \"C\": 1./5., # 1/V\n",
    "        \"offset\": 0, # 0 at 0V\n",
    "        \"gain\": 0.5, # measured over voltage splitter\n",
    "        \"Vmax\": 5, # V output at sensor\n",
    "    },\n",
    "    \"battery\": {\n",
    "        \"C\": 1., # V(batt)/V\n",
    "        \"offset\": 0, # V at 0V\n",
    "        \"gain\": 0.5, # measured over voltage splitter\n",
    "        \"Vmax\": 6.4, # V output at sensor\n",
    "    },\n",
    "    \"radflux\": {\n",
    "        \"C\": 1., # Vsensor/Vmeasured\n",
    "        \"offset\": 0, # V at 0V\n",
    "        \"gain\": 300, # amplified\n",
    "        \"Vmax\": 1500*8*1e-6, # V output at sensor\n",
    "    },\n",
    "}\n",
    "ADCfac = 3.3/1023. # V/counts\n",
    "for var in sensor_config:\n",
    "    sconf = sensor_config[var]\n",
    "    print(f\"{var} scale_factor: {sconf['C']*ADCfac/sconf['gain']}\")\n",
    "    print(f\"{var} add_offset: {sconf['offset']}\")\n",
    "    print(f\"{var} valid_range: [0, {min((1023, int(sconf['Vmax']*sconf['gain']/ADCfac)))}]\")\n",
    "    print()\n",
    "\n",
    "# szen encoding\n",
    "print(f\"scale_factor szen (deg): {180./(36_000)}\")\n",
    "print(f\"add_offset szen: 0.0 \")\n",
    "print(f\"valid_range szen: [0, {36_000}]\")\n",
    "print()\n",
    "# sazi encoding\n",
    "print(f\"scale_factor sazi (deg): {360./(36_000)}\")\n",
    "print(f\"add_offset sazi: 0.0 \")\n",
    "print(f\"valid_range sazi: [0, {36_000}]\")\n",
    "print()\n",
    "# esd encoding\n",
    "print(f\"scale_factor esd (AU): {(1.02-0.98)/(40_000)}\")\n",
    "print(f\"add_offset esd: 0.98 \")\n",
    "print(f\"valid_range esd: [0, {40_000}]\")\n",
    "print()\n",
    "# lat encoding\n",
    "print(f\"scale_factor lat (degree_north): {(90.-(-90.))/180_000_000}\")\n",
    "print(f\"add_offset lat (degree_north): -90.\")\n",
    "print(f\"valid_range lat: [0, 180_000_000]\")\n",
    "print()\n",
    "# lon encoding\n",
    "print(f\"scale_factor lon (degree_east): {(180.-(-180.))/360_000_000}\")\n",
    "print(f\"add_offset lon (degree_east): -180.\")\n",
    "print(f\"valid_range lon: [0, 360_000_000]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.161792800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def add_encoding(ds, vencode=None):\n",
    "    \"\"\"\n",
    "    Set valid_range attribute and encoding to every variable of the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xr.Dataset\n",
    "        Dataset of any processing level. The processing level will be\n",
    "        determined by the global attribute 'processing_level'.\n",
    "    vencode: dict or None\n",
    "        Dictionary of encoding attributes by variable name, will be merged with pyrnet default cfmeta. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        The input dataset but with encoding and valid_range attribute.\n",
    "    \"\"\"\n",
    "    # prepare netcdf encoding\n",
    "    _, vattrs_default, vencode_default = get_cfmeta()\n",
    "\n",
    "    # Add valid range temporary to encoding dict.\n",
    "    # As valid_range is not implemented in xarray encoding,\n",
    "    #  it has to be stored as a variable attribute later.\n",
    "    for k in vencode_default:\n",
    "        if \"valid_range\" not in vencode_default[k]:\n",
    "            continue\n",
    "        vencode_default = assoc_in(vencode_default,\n",
    "                                   [k,'valid_range'],\n",
    "                                   vattrs_default['valid_range'])\n",
    "\n",
    "    # merge input and default with preference on input\n",
    "    if vencode is None:\n",
    "        vencode = vencode_default\n",
    "    else:\n",
    "        a = vencode_default.copy()\n",
    "        b = vencode.copy()\n",
    "        vencode = {}\n",
    "        for k in set(a)-set(b):\n",
    "            vencode.update({k:a[k]})\n",
    "        for k in set(a)&set(b):\n",
    "            vencode.update({k: {**a[k],**b[k]}})\n",
    "        for k in set(b)-set(a):\n",
    "            vencode.update({k:b[k]})\n",
    "\n",
    "    # add encoding to Dataset\n",
    "    for k, v in vencode.items():\n",
    "        if k not in ds:\n",
    "            continue\n",
    "        for ki in [key for key in ds if key.startswith(k)]:\n",
    "            ds[ki].encoding.update(v)\n",
    "        if \"valid_range\" not in vencode[k]:\n",
    "            continue\n",
    "        # add valid_range to variable attributes\n",
    "        for ki in [key for key in ds if key.startswith(k)]:\n",
    "            ds[ki].attrs.update({\n",
    "                'valid_range': vencode[k]['valid_range']\n",
    "            })\n",
    "            \n",
    "    # add encoding to coords\n",
    "    if ds.processing_level=='l1a':\n",
    "        ds[\"gpstime\"].encoding.update({\n",
    "            **vencode[\"time\"],\n",
    "            \"units\": f\"seconds since {np.datetime_as_string(ds.gpstime.data[0], unit='D')}T00:00Z\",\n",
    "        })\n",
    "        ds[\"adctime\"].encoding.update({\n",
    "            **vencode[\"adctime\"],\n",
    "            \"units\": \"milliseconds\"\n",
    "        })\n",
    "        ds[\"station\"].encoding.update({\n",
    "            **vencode[\"station\"]\n",
    "        })\n",
    "    elif ds.processing_level == 'l1b':\n",
    "        ds[\"time\"].encoding.update({\n",
    "            **vencode[\"time\"],\n",
    "            \"units\": f\"seconds since {np.datetime_as_string(ds.time.data[0], unit='D')}T00:00Z\",\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"Dataset has no 'processing_level' attribute.\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## l1a\n",
    "Full resolution unprocessed, uncalibrated data. One data file corresponds to one maintenance period of one PyrNet station.\n",
    "\n",
    "Level *l1a* will be processed from the logger raw data with the following workflow:\n",
    "\n",
    "1. Parse raw logger file\n",
    "    * ```pyrnet.logger.read_records```\n",
    "1. Get maintenance logbook quality flags\n",
    "    * ```pyrnet.reports```\n",
    "1. Get metadata and encoding\n",
    "    * ```pyrnet_cfmeta_l1b.json```\n",
    "1. Make xarray Dataset\n",
    "1. Add variable and global attributes and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.175844800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "gattrs, vattrs, vencode = get_cfmeta(config)\n",
    "\n",
    "# update encoding with sensor config for l1a\n",
    "sconfig = get_sensor_config()\n",
    "sencoding = calc_encoding(sconfig, ADCV=3.3, ADCbits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.190504300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from toolz import update_in\n",
    "for var,enc in sencoding.items():\n",
    "    for k,v in enc.items():\n",
    "        if k==\"valid_range\":\n",
    "            vattrs = assoc_in(vattrs, [var,k], v)\n",
    "        else:\n",
    "            vencode = assoc_in(vencode, [var,k], v)\n",
    "vencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.209983600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_l1a(\n",
    "        fname : str,\n",
    "        *,\n",
    "        station: int,\n",
    "        report: dict|pd.DataFrame|None,\n",
    "        date_of_measure : np.datetime64 = np.datetime64(\"now\"),\n",
    "        config: dict|None = None,\n",
    "        sconfig: dict|None = None,\n",
    "        global_attrs: dict|None = None\n",
    ") -> xr.Dataset|None:\n",
    "    \"\"\"\n",
    "    Read logger raw file and parse it to xarray Dataset. Thereby, attributes and names are defined via cfmeta.json file and sun position values are calculated and added.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        Path and filename of the raw logger file.\n",
    "    station: int\n",
    "        PyrNet station box number.\n",
    "    report: dict\n",
    "        Parsed maintenance report, see reports.ipynb\n",
    "    bins: int\n",
    "        Number of desired bins per day. The default is 86400, which result in\n",
    "        mean values of 1 second steps per day. Maximum resolution is 86400000.\n",
    "    date_of_measure: float, datetime or datetime64\n",
    "        A rough date of measure  to account for GPS week rollover. If measured in 2019, day resolution is recommended, before 2019 annual resolution, 2020 onwards not required. If float, interpreted as Julian day from 2000-01-01T12:00. the default is np.datetime64(\"now\").\n",
    "    config: dict\n",
    "        Stores processing specific configuration.\n",
    "            * cfjson -> path to cfmeta.json, the default is \"../share/pyrnet_cfmeta.json\"\n",
    "            * stripminutes -> number of minutes to be stripped from the data at start and end,\n",
    "                the default is 5.\n",
    "    sconfig: dict\n",
    "        Config for ADC and amplifier for each sensor. The default is \"../share/pyrnet_sensor_config.json\"\n",
    "    global_attrs: dict\n",
    "        Additional global attributes for the Dataset. (Overrides cfmeta.json attributes)\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Raw Logger data for one measurement periode.\n",
    "    \"\"\"\n",
    "    ADCV = 3.3\n",
    "    ADCbits = 10\n",
    "    \n",
    "    # load and merge  default config\n",
    "    config = get_config(config)\n",
    "    gattrs, vattrs, vencode = get_cfmeta(config)\n",
    "    \n",
    "    # update encoding with sensor config for l1a\n",
    "    sconfig = get_sensor_config(sconfig)\n",
    "    sencoding = calc_encoding(sconfig, ADCV=ADCV, ADCbits=ADCbits)\n",
    "    # update encoding with sensor config for l1a\n",
    "    for var, enc in sencoding.items():\n",
    "        for k, v in enc.items():\n",
    "            if k in [\"valid_range\", \"units\"]:\n",
    "                vattrs = assoc_in(vattrs, [var, k], v)\n",
    "            else:\n",
    "                vencode = assoc_in(vencode, [var, k], v)\n",
    "                \n",
    "    # update additional global attributes\n",
    "    if global_attrs is not None:\n",
    "        gattrs.update(global_attrs)\n",
    "\n",
    "    date_of_measure = pyrutils.to_datetime64(date_of_measure)\n",
    "\n",
    "    # 1. Parse raw file\n",
    "    rec_adc, rec_gprmc = pyrlogger.read_records(fname=fname, date_of_measure=date_of_measure)\n",
    "\n",
    "    if type(rec_adc)==bool or len(rec_gprmc.time)<3:\n",
    "        logger.debug(\"Failed to load the data from the file, because of not enough stable GPS data, or file is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Get ADC time\n",
    "    adctime = pyrlogger.get_adc_time(rec_adc)\n",
    "\n",
    "    # ADC to Volts\n",
    "    # Drop time and internal battery sensor output (columns 0 and 1)\n",
    "    adc_volts = ADCV * rec_adc[:,2:] / float(2**ADCbits - 1)\n",
    "\n",
    "    # 2. Get Logbook maintenance quality flags\n",
    "    key = f\"{station:03d}\"\n",
    "    if report is None:\n",
    "        logger.warning(\"No report available!\")\n",
    "        report = {}\n",
    "    if isinstance(report, pd.DataFrame):\n",
    "        logger.info(f\"Parsing report at date {rec_gprmc.time[-1]}\")\n",
    "        report = pyrreports.parse_report(report,\n",
    "                                date_of_maintenance=rec_gprmc.time[-1])\n",
    "\n",
    "    if key not in report:\n",
    "        logger.warning(f\"No report for station {station} available.\")\n",
    "        warnings.warn(f\"No report for station {station} available.\")\n",
    "        qc_main = pyrreports.get_qcflag(4,3)\n",
    "        qc_extra = pyrreports.get_qcflag(4,3)\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_general\"], \"No maintenance report!\")\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_general\"], \"No maintenance report!\")\n",
    "    else:\n",
    "        qc_main = pyrreports.get_qcflag(\n",
    "            qc_clean=report[key]['clean'],\n",
    "            qc_level=report[key]['align']\n",
    "        )\n",
    "        qc_extra = pyrreports.get_qcflag(\n",
    "            qc_clean=report[key]['clean2'],\n",
    "            qc_level=report[key]['align2']\n",
    "        )\n",
    "        # add qc notes\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_general\"], report[key][\"note_general\"])\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_general\"], report[key][\"note_general\"])\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_clean\"], report[key][\"note_clean\"])\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_clean\"], report[key][\"note_clean2\"])\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_level\"], report[key][\"note_align\"])\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_level\"], report[key][\"note_align2\"])\n",
    "\n",
    "    # 3. Add global meta data\n",
    "    now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "    gattrs.update({\n",
    "        'processing_level': 'l1a',\n",
    "        'product_version': pyrnet_version,\n",
    "        'history': f'{now.isoformat()}: Generated level l1a  by pyrnet version {pyrnet_version}; ',\n",
    "    })\n",
    "    # add site information\n",
    "    if config['sites'] is not None:\n",
    "        sites = pyrutils.read_json(config['file_site'])[config['sites']]\n",
    "        if key in sites:\n",
    "            gattrs.update({ \"site\" : sites[key]})\n",
    "\n",
    "    # add gti angles\n",
    "    # default horizontal\n",
    "    vattrs = assoc_in(vattrs, [\"gti\",\"hangle\"], 0.)\n",
    "    vattrs = assoc_in(vattrs, [\"gti\",\"vangle\"], 0.)\n",
    "    # update with angles from mapping file\n",
    "    if config['gti_angles'] is not None:\n",
    "        gti_angles = pyrutils.read_json(config['file_gti_angles'])[config['gti_angles']]\n",
    "        if key in gti_angles:\n",
    "            hangle = np.nan if gti_angles[key][0] is None else gti_angles[key][0]\n",
    "            vangle = np.nan if gti_angles[key][1] is None else gti_angles[key][1]\n",
    "            vattrs = assoc_in(vattrs, [\"gti\",\"hangle\"], hangle)\n",
    "            vattrs = assoc_in(vattrs, [\"gti\",\"vangle\"], vangle)\n",
    "\n",
    "    if adc_volts.shape[1]<5: # gti data is not available\n",
    "        adc_volts = np.concatenate((adc_volts,-1*np.ones(adc_volts.shape[0])[:,None]),axis=1)\n",
    "\n",
    "    # 8. Make xarray Dataset\n",
    "    values = {}\n",
    "    for k,v in sconfig.items():\n",
    "        offset = sconfig[k][\"offset\"]\n",
    "        gain = sconfig[k][\"gain\"]\n",
    "        C = sconfig[k][\"C\"]\n",
    "        iadc = sconfig[k][\"iadc\"]\n",
    "        volts = adc_volts[:,iadc][:,None]\n",
    "        values.update(\n",
    "            {k: offset + C*volts/gain}\n",
    "        )\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            \"ghi\": ((\"adctime\",\"station\"), values[\"ghi\"]), # [V]\n",
    "            \"gti\": ((\"adctime\",\"station\"), values[\"gti\"]), # [V]\n",
    "            \"ta\": ((\"adctime\",\"station\"), values[\"ta\"]), # [K]\n",
    "            \"rh\": ((\"adctime\",\"station\"), values[\"rh\"]), # [-]\n",
    "            \"battery_voltage\": ((\"adctime\",\"station\"), values[\"battery_voltage\"]), # [V]\n",
    "            \"lat\": ((\"gpstime\",\"station\"), rec_gprmc.lat[:,None]), # [degN]\n",
    "            \"lon\": ((\"gpstime\",\"station\"), rec_gprmc.lon[:,None]), # [degE]\n",
    "            \"ghi_qc\": (\"station\", [qc_main]),\n",
    "            \"gti_qc\": (\"station\", [qc_extra]),\n",
    "            \"iadc\": ((\"gpstime\", \"station\"), rec_gprmc.iadc[:,None])\n",
    "        },\n",
    "        coords={\n",
    "            \"adctime\": (\"adctime\", adctime.astype('timedelta64[ns]')),\n",
    "            \"gpstime\": (\"gpstime\", rec_gprmc.time.astype('datetime64[ns]')),\n",
    "            \"station\": (\"station\", [station]),\n",
    "        },\n",
    "        attrs=gattrs\n",
    "    )\n",
    "\n",
    "    # drop ocurance of douplicate gps values\n",
    "    ds = ds.drop_duplicates(\"gpstime\")\n",
    "\n",
    "    # add global coverage attributes\n",
    "    ds = update_coverage_meta(ds, timevar=\"gpstime\")\n",
    "\n",
    "    # add attributes to Dataset\n",
    "    for k,v in vattrs.items():\n",
    "        if k not in ds.keys():\n",
    "            continue\n",
    "        ds[k].attrs = v\n",
    "\n",
    "    # add encoding to Dataset\n",
    "    ds = add_encoding(ds, vencode)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.222949200Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "fn_report = \"../../example_data/results-survey224783.csv\"\n",
    "fn_data = \"../../example_data/Pyr9_000.bin\"\n",
    "\n",
    "fn_cfmeta = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_cfmeta.json\")\n",
    "\n",
    "\n",
    "# parse report\n",
    "df_report = pyrreports.get_responses(fn=\"../../example_data/results-survey224783.csv\")\n",
    "report = pyrreports.parse_report(df_report,\n",
    "                              date_of_maintenance=np.datetime64(\"2023-05-08T12:00\"))\n",
    "# read logger file to xarray\n",
    "ds = to_l1a(\n",
    "    fname=fn_data,\n",
    "    station=1, # actually test data is from station 9, but test reports are for station 1 and 2 only\n",
    "    # bins=86400, # seconds resolution\n",
    "    report=report,\n",
    "    config={\"file_cfmeta\": fn_cfmeta, \"stripminutes\": 0},\n",
    "    global_attrs={\"TESTNOTE\": \"This is a test note.\"}\n",
    ")\n",
    "\n",
    "ds.to_netcdf(\"../../example_data/to_l1a_output.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.231269200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cfchecker.cfchecks\n",
    "init = cfchecker.cfchecks.CFChecker()\n",
    "res = init.checker(\"../../example_data/to_l1a_output.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.246893Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "import netCDF4\n",
    "netCDF4.Dataset(\"../../example_data/to_l1a_output.nc\",'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "## l1b\n",
    "Resampled, calibrated data as daily files per station. Resampling is per default to one second, but can be configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.262514300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = \"../../example_data/to_l1a_output.nc\"\n",
    "config = {\n",
    "    \"l1bfreq\":\"1s\",\n",
    "    \"stripminutes\":5,\n",
    "    \"average_latlon\":True,\n",
    "}\n",
    "\n",
    "config = get_config(config)\n",
    "gattrs, vattrs, vencode = get_cfmeta(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The l1b data is processed from the l1b data with the following workflow:\n",
    "\n",
    "1. Read l1a netcdf\n",
    "2. Sync GPS to ADC time\n",
    "    * ```sync_adc_time```\n",
    "3. Make dataset with new time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.282228100Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 1. Load l1a data\n",
    "ds_l1a = xr.open_dataset(fname)\n",
    "# check correct file\n",
    "if ds_l1a.processing_level != \"l1a\":\n",
    "    raise ValueError(f\"{fname} is not a l1a file.\")\n",
    "\n",
    "\n",
    "# 2. Sync GPS to ADC time\n",
    "adctime = pyrlogger.sync_adc_time(\n",
    "    adctime = ds_l1a.adctime.values,\n",
    "    gpstime = ds_l1a.gpstime.values,\n",
    "    iadc = ds_l1a.iadc.squeeze().values.astype(int),\n",
    "    check_results= False\n",
    ")\n",
    "\n",
    "# 3. Create new dataset (l1b) with synced time\n",
    "\n",
    "ds_l1b = ds_l1a.drop_dims('gpstime')\n",
    "ds_l1b = ds_l1b.drop_vars(['ghi_qc','gti_qc']) # keep only time dependend variables\n",
    "ds_l1b = ds_l1b.assign({'time': ('adctime', adctime)})\n",
    "ds_l1b = ds_l1b.swap_dims({\"adctime\":\"time\"})\n",
    "ds_l1b = ds_l1b.drop_vars(\"adctime\")\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.298611800Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Drop first and last X minutes of data to avoid bad data due to maintenance\n",
    "\n",
    "```{note}\n",
    "config.json -> \"stripminutes\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.309362Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 4. Drop first and last <stripminutes> minutes of data to avoid bad data due to maintenance\n",
    "\n",
    "# stripminutes = np.timedelta64(int(config['stripminutes']), 'm')\n",
    "# for now use some seconds for demonstration\n",
    "stripminutes = np.timedelta64(2, 's')\n",
    "tslice = slice(ds_l1b.time.values[0] + stripminutes,\n",
    "               ds_l1b.time.values[-1] - stripminutes)\n",
    "ds_l1b = ds_l1b.sel(time=tslice)\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Calibrate radiation flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.330216800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. rad flux calibration\n",
    "box = ds_l1b.station.values[0]\n",
    "boxnumber, serial, cfac = pyrnet.meta_lookup(\n",
    "    ds_l1b.time.values[0],\n",
    "    box=box,\n",
    "    cfile=config['file_calibration'],\n",
    "    mapfile=config['file_mapping'],\n",
    ")\n",
    "\n",
    "print(f\"Meta Lookup:\")\n",
    "print(f\">> Box={box}\")\n",
    "print(f\">> serial(s)={serial}\")\n",
    "print(f\"calibration factor(s)={cfac}\")\n",
    "\n",
    "\n",
    "# calibrate radiation flux with gain=300\n",
    "for i, radflx in enumerate(config['radflux_varname']):\n",
    "    if cfac[i] is None:\n",
    "        # drop if calibration/instrument don't exist (probably secondary pyranometer).\n",
    "        ds_l1b = ds_l1b.drop_vars([var for var in ds_l1b if radflx in var])\n",
    "        continue\n",
    "    ds_l1b[radflx].values = ds_l1b[radflx].values*1e6/cfac[i] # V -> W m-2\n",
    "    ds_l1b[radflx].attrs['units'] = \"W m-2\",\n",
    "    ds_l1b[radflx].attrs.update({\n",
    "        \"units\": \"W m-2\",\n",
    "        \"serial\": serial[i],\n",
    "        \"calibration_factor\": cfac[i]\n",
    "    })\n",
    "    # ds_l1b[radflx].encoding.update({\n",
    "    #     'scale_factor': ds_l1b[radflx].encoding['scale_factor']*1e6/cfac[i]\n",
    "    # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.343949200Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Apply appropriate binning to ADC values\n",
    "    * ```pyrnet.data.resample```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.361314800Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 6. resample to desired resolution\n",
    "# --- This method is replaced with the data.resample function ---\n",
    "# ds_l1b1 = pyrlogger.resample_mean(ds_l1b,freq=config['l1bfreq'])\n",
    "# ds_l1b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.371363100Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 6. resample to desired resolution\n",
    "# save station coordinate\n",
    "station_dim = {\"station\": ds_l1b[\"station\"].values}\n",
    "\n",
    "# resample on time dimension with specified methods\n",
    "methods = ['mean'] + config[\"l1b_resample_stats\"]\n",
    "res = resample(\n",
    "    ds_l1b.squeeze().drop_vars(\"station\"), # drop station coordinate and variable\n",
    "    freq=config['l1bfreq'],\n",
    "    methods=methods,\n",
    "    kwargs=dict(skipna=True)\n",
    ")\n",
    "\n",
    "# add standard names for new variables\n",
    "ds_l1b = res[0]\n",
    "for i, method in enumerate(methods[1:]):\n",
    "    for var in config[\"radflux_varname\"]:\n",
    "        ds_l1b[f\"{var}_{method}\"] = res[i+1][var]\n",
    "        ds_l1b[f\"{var}_{method}\"].attrs.update({\n",
    "            \"standard_name\": f\"{method}_\"+ds_l1b[f\"{var}_{method}\"].attrs[\"standard_name\"]\n",
    "        })\n",
    "\n",
    "# add station dimension back again\n",
    "ds_l1b = ds_l1b.expand_dims(station_dim, axis=-1)\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.381342500Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " * stretch valid_range to not lose resolution due to averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.394090900Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# stretch valid range to not lose resolution due to averaging\n",
    "# ds_l1b = stretch_resolution(ds_l1b)\n",
    "# ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.425072800Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. Interpolate GPS coordinates to bin time\n",
    "    * ```xarray.interp```\n",
    "    ```{note}\n",
    "    At this point the descision to whether store geocoordinates in full time resolution or as mean over the time interval is made.\n",
    "\n",
    "    This is configured in config.json -> \"average_latlon\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.440594700Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "\n",
    "# 7. Interpolate GPS coordinates to bin time\n",
    "ds_gps = ds_l1a.drop_dims(\"adctime\")\n",
    "ds_gps = ds_gps.drop_vars(['iadc'])\n",
    "\n",
    "\n",
    "# Decide whether geo coordinates should be averaged or not\n",
    "\n",
    "# if config['average_latlon']:\n",
    "ds_gps_avg = ds_gps.mean('gpstime',skipna=True)\n",
    "ds_l1b_avg = xr.merge((ds_l1b,ds_gps_avg))\n",
    "\n",
    "# else:\n",
    "ds_gps = ds_gps.interp(gpstime=ds_l1b.time,\n",
    "                       kwargs={\"bounds_error\":False, \"fill_value\":np.nan})\n",
    "ds_gps = ds_gps.drop_vars(\"gpstime\")\n",
    "\n",
    "ds_l1b = xr.merge((ds_l1b,ds_gps))\n",
    "\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.440594700Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. Calculate sun position\n",
    "    * ```trosat.sunpos```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.456223300Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 8. Calc and add sun position\n",
    "\n",
    "szen, sazi = sp.sun_angles(\n",
    "    time=ds_l1b.time.values[:,None],\n",
    "    lat=ds_l1b.lat.values,\n",
    "    lon=ds_l1b.lon.values\n",
    ")\n",
    "\n",
    "\n",
    "szen_avg, sazi_avg = sp.sun_angles(\n",
    "    time=ds_l1b_avg.time.values[:,None],\n",
    "    lat=ds_l1b_avg.lat.values,\n",
    "    lon=ds_l1b_avg.lon.values\n",
    ")\n",
    "\n",
    "szen  = szen.squeeze()\n",
    "sazi = sazi.squeeze()\n",
    "szen_avg = szen_avg.squeeze()\n",
    "sazi_avg = sazi_avg.squeeze()\n",
    "\n",
    "\n",
    "esd = np.mean(sp.earth_sun_distance(ds_l1b.time.values))\n",
    "\n",
    "print('szen (avg latlon):', szen_avg)\n",
    "print('szen:', szen)\n",
    "print('sazi  (avg latlon):', sazi_avg)\n",
    "print('sazi:', sazi)\n",
    "print('Earth-Sun Distance:',esd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.460229800Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "ds_l1b = ds_l1b.assign(\n",
    "    {\n",
    "        \"szen\": ((\"time\", \"station\"), szen[:,None]),\n",
    "        \"sazi\": ((\"time\", \"station\"), sazi[:,None]),\n",
    "        \"esd\": (\"station\", [esd])\n",
    "    }\n",
    ")\n",
    "for key in ['szen', 'sazi','esd']:\n",
    "    ds_l1b[key].attrs.update(vattrs[key])\n",
    "    # ds_l1b[key].encoding.update(vencode[key])\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. Update variables and global attributes and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.460229800Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# add global coverage attributes\n",
    "ds_l1b = update_coverage_meta(ds_l1b, timevar=\"time\")\n",
    "\n",
    "ds_l1b.attrs[\"processing_level\"] = 'l1b'\n",
    "now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "ds_l1b.attrs[\"history\"] = ds_l1b.history + f\"{now.isoformat()}: Generated level l1b  by pyrnet version {pyrnet_version}; \"\n",
    "\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.473649600Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.473649600Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# ds_l1b.to_netcdf(\"../../testnb/to_l1b_output.nc\",\n",
    "#                  encoding={'time':{'dtype':'float64'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.483669400Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = xr.load_dataset(\"../../testnb/to_l1b_output.nc\").sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.488178Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "#|dropcode\n",
    "def to_l1b(\n",
    "        fname: str,\n",
    "        *,\n",
    "        config: dict | None = None,\n",
    "        global_attrs: dict | None = None,\n",
    "        check_adc_sync: bool = True\n",
    ") -> xr.Dataset|None:\n",
    "\n",
    "    config = get_config(config)\n",
    "    gattrs, vattrs, vencode = get_cfmeta(config)\n",
    "\n",
    "    if global_attrs is not None:\n",
    "        gattrs.update(global_attrs)\n",
    "\n",
    "    # 1. Load l1a data\n",
    "    ds_l1a = xr.open_dataset(fname)\n",
    "    # check correct file\n",
    "    if ds_l1a.processing_level != \"l1a\":\n",
    "        logger.warning(f\"{fname} is not a l1a file. Skip.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Sync GPS to ADC time\n",
    "    adctime = pyrlogger.sync_adc_time(\n",
    "        adctime = ds_l1a.adctime.values,\n",
    "        gpstime = ds_l1a.gpstime.values,\n",
    "        iadc = ds_l1a.iadc.squeeze().values.astype(int),\n",
    "        check_results = check_adc_sync\n",
    "    )\n",
    "    \n",
    "    if adctime is None:\n",
    "        logger.warning(f\"Could not fit GPS to ADC time for file {fname}. Skip.\")\n",
    "        return None\n",
    "\n",
    "    # 3. Create new dataset (l1b)\n",
    "    ds_l1b = ds_l1a.drop_dims('gpstime')\n",
    "    ds_l1b = ds_l1b.drop_vars(['ghi_qc','gti_qc']) # keep only time dependent variables\n",
    "    ds_l1b = ds_l1b.assign({'time': ('adctime', adctime)})\n",
    "    ds_l1b = ds_l1b.swap_dims({\"adctime\":\"time\"})\n",
    "    ds_l1b = ds_l1b.drop_vars(\"adctime\")\n",
    "\n",
    "    ds_l1b[\"time\"].encoding.update({\n",
    "        \"dtype\": 'float64',\n",
    "        \"units\": f\"seconds since {np.datetime_as_string(ds_l1b.time.data[0], unit='D')}T00:00Z\",\n",
    "    })\n",
    "    logger.info(f\"Dataset time coverage before strip: {ds_l1b.time.values[0]} - {ds_l1b.time.values[-1]}\")\n",
    "\n",
    "    # 4. Drop first and last <stripminutes> minutes of data to avoid bad data due to maintenance\n",
    "    stripminutes = np.timedelta64(int(config['stripminutes']), 'm')\n",
    "    if (ds_l1b.time.values[0] + 3*stripminutes) > ds_l1b.time.values[-1]:\n",
    "        logger.warning(f\"{fname} has not enough data. Skip.\")\n",
    "        return None\n",
    "\n",
    "    ds_l1b = ds_l1b.isel(time=ds_l1b.time>ds_l1b.time.values[0] + stripminutes)\n",
    "    ds_l1b = ds_l1b.isel(time=ds_l1b.time<ds_l1b.time.values[-1] - stripminutes)\n",
    "    if ds_l1b.time.size < 10:\n",
    "        logger.warning(f\"{fname} has not enough data, after strip. Skip.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    logger.info(f\"Dataset time coverage after strip: {ds_l1b.time.values[0]} - {ds_l1b.time.values[-1]}\")\n",
    "\n",
    "    # 5. rad flux calibration\n",
    "    box = ds_l1b.station.values[0]\n",
    "    boxnumber, serial, cfac = pyrnet.meta_lookup(\n",
    "        ds_l1b.time.values[0],\n",
    "        box=box,\n",
    "        cfile=config['file_calibration'],\n",
    "        mapfile=config['file_mapping'],\n",
    "    )\n",
    "    logger.info(f\"Meta Lookup:\")\n",
    "    logger.info(f\">> Box={box}\")\n",
    "    logger.info(f\">> serial(s)={serial}\")\n",
    "    logger.info(f\">> calibration factor(s)={cfac}\")\n",
    "\n",
    "\n",
    "    # calibrate radflux variables\n",
    "    for i, radflx in enumerate(config['radflux_varname']):\n",
    "        if cfac[i] is None:\n",
    "            # drop if calibration/instrument don't exist (probably secondary pyranometer).\n",
    "            ds_l1b = ds_l1b.drop_vars([var for var in ds_l1b if radflx in var])\n",
    "            continue\n",
    "        ds_l1b[radflx].values = ds_l1b[radflx].values*1e6/(cfac[i]) # V -> W m-2\n",
    "        ds_l1b[radflx].attrs.update({\n",
    "            \"serial\": serial[i],\n",
    "            \"calibration_factor\": cfac[i],\n",
    "            \"calibration_function\": \"flux (W m-2) = flux (V) * calibration_factor (W m-2 V-1)\",\n",
    "        })\n",
    "\n",
    "\n",
    "    # 6. resample to desired resolution\n",
    "    # save station coordinate\n",
    "    station_dim = {\"station\": ds_l1b[\"station\"].values}\n",
    "    \n",
    "    # resample on time dimension with specified methods\n",
    "    methods = ['mean'] + config[\"l1b_resample_stats\"]\n",
    "    res = resample(\n",
    "        ds_l1b.squeeze().drop_vars(\"station\"), # drop station coordinate and variable\n",
    "        freq=config['l1bfreq'],\n",
    "        methods=methods,\n",
    "        kwargs=dict(skipna=True)\n",
    "    )\n",
    "    \n",
    "    # add standard names for new variables\n",
    "    # apply for variables if both in config['radflux_varname'] and ds_l1b.keys()\n",
    "    radflux_vars = list(set(config['radflux_varname'])&set(ds_l1b.keys()))\n",
    "    ds_l1b = res[0]\n",
    "    for i, method in enumerate(methods[1:]):\n",
    "        for var in radflux_vars:\n",
    "            ds_l1b[f\"{var}_{method}\"] = res[i+1][var]\n",
    "            ds_l1b[f\"{var}_{method}\"].attrs.update({\n",
    "                \"standard_name\": f\"{method}_\"+ds_l1b[f\"{var}_{method}\"].attrs[\"standard_name\"]\n",
    "            })\n",
    "    \n",
    "    # add station dimension back again\n",
    "    ds_l1b = ds_l1b.expand_dims(station_dim, axis=-1)\n",
    "\n",
    "    # 7. Interpolate GPS coordinates to l1b time\n",
    "    ds_gps = ds_l1a.drop_dims(\"adctime\")\n",
    "    ds_gps = ds_gps.drop_vars(['iadc'])\n",
    "\n",
    "    # Decide whether geo coordinates should be averaged or not\n",
    "    if config['average_latlon']:\n",
    "        ds_gps = ds_gps.mean('gpstime', skipna=True, keep_attrs=True)\n",
    "    else:\n",
    "        ds_gps = ds_gps.interp(gpstime=ds_l1b.time)\n",
    "        ds_gps = ds_gps.drop_vars(\"gpstime\")\n",
    "\n",
    "    ds_l1b = xr.merge((ds_l1b,ds_gps))\n",
    "\n",
    "    # 8. Calc and add sun position\n",
    "    szen, sazi = sp.sun_angles(\n",
    "        time=ds_l1b.time.values[:,None], # line up with coordinates to keep dependence on time only\n",
    "        lat=ds_l1b.lat.values,\n",
    "        lon=ds_l1b.lon.values\n",
    "    )\n",
    "    szen  = szen.squeeze()\n",
    "    sazi = sazi.squeeze()\n",
    "\n",
    "    esd = np.mean(sp.earth_sun_distance(ds_l1b.time.values))\n",
    "\n",
    "    ds_l1b = ds_l1b.assign(\n",
    "        {\n",
    "            \"szen\": ((\"time\", \"station\"), szen[:,None]),\n",
    "            \"sazi\": ((\"time\", \"station\"), sazi[:,None]),\n",
    "            \"esd\": (\"station\", [esd])\n",
    "        }\n",
    "    )\n",
    "    # update attributes and encoding\n",
    "    for key in ['szen', 'sazi','esd']:\n",
    "        ds_l1b[key].attrs.update(vattrs[key])\n",
    "\n",
    "    # add global coverage attributes\n",
    "    ds_l1b = update_coverage_meta(ds_l1b, timevar=\"time\")\n",
    "    ds_l1b.attrs[\"processing_level\"] = 'l1b'\n",
    "    now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "    ds_l1b.attrs[\"history\"] = ds_l1b.history + f\"{now.isoformat()}: Generated level l1b  by pyrnet version {pyrnet_version}; \"\n",
    "\n",
    "    # update encoding\n",
    "    ds_l1b = add_encoding(ds_l1b, vencode=vencode)\n",
    "\n",
    "    return ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.488178Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "fname = \"../../example_data/to_l1a_output.nc\"\n",
    "config = {\n",
    "    \"l1bfreq\":\"1s\",\n",
    "    \"stripminutes\":0,\n",
    "    \"average_latlon\":True,\n",
    "    \"l1b_resample_stats\": [\"min\", \"max\", \"std\"]\n",
    "}\n",
    "\n",
    "ds_l1b = to_l1b(fname=fname, config=config, check_adc_sync=False)\n",
    "ds_l1b.to_netcdf(\"../../example_data/to_l1b_output.nc\",\n",
    "                 encoding={'time':{'dtype':'float64'}}) # use float64 and not int64 for opendap2 compatibility\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.488178Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cfchecker.cfchecks\n",
    "init = cfchecker.cfchecks.CFChecker()\n",
    "res = init.checker(\"../../example_data/to_l1b_output.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.503812900Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "import netCDF4\n",
    "netCDF4.Dataset(\"../../example_data/to_l1b_output.nc\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-31T15:31:29.510368200Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell",
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Export module\n",
    "# Requires *nbdev* to export and update the *../lib/logger.py* module\n",
    "import nbdev.export\n",
    "import nbformat as nbf\n",
    "name = \"data\"\n",
    "\n",
    "# Export python module\n",
    "nbdev.export.nb_export( f\"{name}.ipynb\" ,f\"../../src/pyrnet\")\n",
    "\n",
    "# Export to docs\n",
    "ntbk = nbf.read(f\"{name}.ipynb\", nbf.NO_CONVERT)\n",
    "\n",
    "text_search_dict = {\n",
    "    \"#|hide\": \"remove-cell\",  # Remove the whole cell\n",
    "    \"#|dropcode\": \"hide-input\",  # Hide the input w/ a button to show\n",
    "    \"#|dropout\": \"hide-output\"  # Hide the output w/ a button to show\n",
    "}\n",
    "for cell in ntbk.cells:\n",
    "    cell_tags = cell.get('metadata', {}).get('tags', [])\n",
    "    for key, val in text_search_dict.items():\n",
    "            if key in cell['source']:\n",
    "                if val not in cell_tags:\n",
    "                    cell_tags.append(val)\n",
    "    if len(cell_tags) > 0:\n",
    "        cell['metadata']['tags'] = cell_tags\n",
    "    nbf.write(ntbk, f\"../../docs/source/nbs/{name}.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
