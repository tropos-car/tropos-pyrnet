{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:30.134432600Z",
     "start_time": "2024-01-29T14:31:29.414984200Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data level\n",
    "\n",
    "Reading and advance in data processing levels to be stored in netcdf database format.\n",
    "\n",
    "```{note}\n",
    "raw > l1a > l1b > l1c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:30.875331700Z",
     "start_time": "2024-01-29T14:31:29.472495Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import logging\n",
    "from toolz import assoc_in\n",
    "import pkg_resources as pkg_res\n",
    "import warnings\n",
    "\n",
    "from trosat import sunpos as sp\n",
    "\n",
    "import pyrnet as pyrnet_main\n",
    "pyrnet_version = pyrnet_main.__version__\n",
    "from pyrnet import pyrnet\n",
    "from pyrnet import utils as pyrutils\n",
    "from pyrnet import logger as pyrlogger\n",
    "from pyrnet import reports as pyrreports\n",
    "\n",
    "# logging setup\n",
    "logging.basicConfig(\n",
    "    filename='pyrnet.log',\n",
    "    encoding='utf-8',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(name)s %(levelname)s:%(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.035944400Z",
     "start_time": "2024-01-29T14:31:29.487043100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## netCDF dataset operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.066848600Z",
     "start_time": "2024-01-29T14:31:29.506526800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def update_coverage_meta(ds,timevar='time'):\n",
    "    \"\"\"Update global attributes related to geospatial and time coverage\n",
    "    \"\"\"\n",
    "    duration = ds[timevar].values[-1] - ds[timevar].values[0]\n",
    "    resolution = np.mean(np.diff(ds[timevar].values))\n",
    "    now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "    gattrs = {\n",
    "        'date_created': now.isoformat(),\n",
    "        'geospatial_lat_min': np.nanmin(ds.lat.values),\n",
    "        'geospatial_lat_max': np.nanmax(ds.lat.values),\n",
    "        'geospatial_lat_units': 'degN',\n",
    "        'geospatial_lon_min': np.nanmin(ds.lon.values),\n",
    "        'geospatial_lon_max': np.nanmax(ds.lon.values),\n",
    "        'geospatial_lon_units': 'degE',\n",
    "        'time_coverage_start': pd.to_datetime(ds[timevar].values[0]).isoformat(),\n",
    "        'time_coverage_end': pd.to_datetime(ds[timevar].values[-1]).isoformat(),\n",
    "        'time_coverage_duration': pd.to_timedelta(duration).isoformat(),\n",
    "        'time_coverage_resolution': pd.to_timedelta(resolution).isoformat(),\n",
    "    }\n",
    "    ds.attrs.update(gattrs)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.096196100Z",
     "start_time": "2024-01-29T14:31:29.523721200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def stretch_resolution(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\" Stretch variable resolution to full integer size,\n",
    "    to not lose resolution after averaging ADC count data.\"\"\"\n",
    "    for var in ds:\n",
    "        if \"scale_factor\" not in ds[var].encoding:\n",
    "            continue\n",
    "        if \"valid_range\" not in ds[var].attrs:\n",
    "            continue\n",
    "        dtype = ds[var].encoding['dtype']\n",
    "        valid_range = ds[var].valid_range\n",
    "        int_limit = np.iinfo(dtype).max\n",
    "        scale_factor = ds[var].encoding['scale_factor']\n",
    "        scale_factor_mod = int((int_limit-1)/valid_range[1])\n",
    "        ds[var].encoding.update({\n",
    "            \"scale_factor\": scale_factor / scale_factor_mod,\n",
    "            \"_FillValue\": int_limit,\n",
    "        })\n",
    "        ds[var].attrs.update({\n",
    "            \"valid_range\": valid_range * scale_factor_mod\n",
    "        })\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.097157900Z",
     "start_time": "2024-01-29T14:31:29.538439700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def merge_ds(ds1,ds2,timevar=\"time\"):\n",
    "    \"\"\"Merge two datasets along the time dimension.\n",
    "    \"\"\"\n",
    "    if ds1[timevar].equals(ds2[timevar]):\n",
    "        logging.info(\"Overwrite existing file.\")\n",
    "        return ds2\n",
    "    logging.info(\"Merge with existing file.\")\n",
    "\n",
    "    ## overwrite non time dependent variables\n",
    "    overwrite_vars = [ v for v in ds1 if timevar not in ds1[v].dims ]\n",
    "\n",
    "    ## merge both datasets\n",
    "    ds_new=ds1.merge(ds2,\n",
    "                     compat='no_conflicts',\n",
    "                     overwrite_vars=overwrite_vars)\n",
    "\n",
    "    # add global coverage attributes\n",
    "    ds_new.attrs.update({'merged':1})\n",
    "\n",
    "    # add encoding again\n",
    "    ds_new = add_encoding(ds_new)\n",
    "    return ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.202005400Z",
     "start_time": "2024-01-29T14:31:29.559105400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_netcdf(ds,fname, timevar=\"time\"):\n",
    "    \"\"\"xarray to netcdf, but merge if exist\n",
    "    \"\"\"\n",
    "    # merge if necessary\n",
    "    if os.path.exists(fname):\n",
    "        ds1 = xr.open_dataset(fname)\n",
    "        ds = merge_ds(ds1,ds, timevar=timevar)\n",
    "        ds1.close()\n",
    "        os.remove(fname)\n",
    "\n",
    "    # save to netCDF4\n",
    "    ds = update_coverage_meta(ds, timevar=timevar)\n",
    "    ds.to_netcdf(fname,\n",
    "                 encoding={timevar:{'dtype':'float64'}}) # for OpenDAP 2 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.334522700Z",
     "start_time": "2024-01-29T14:31:29.576455200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def resample(ds, freq, methods='mean', kwargs={}):\n",
    "    \"\"\" Resample xarray dataset using pandas for speed.\n",
    "    https://github.com/pydata/xarray/issues/4498#issuecomment-706688398\n",
    "    \"\"\"\n",
    "    if isinstance(methods,str):\n",
    "        methods = [methods]\n",
    "\n",
    "    dsr = ds.to_dataframe().resample(freq)\n",
    "    dsouts = []\n",
    "    for method in methods:\n",
    "        # what we want (quickly), but in Pandas form\n",
    "        df_h = dsr.apply(method)\n",
    "        # rebuild xarray dataset with attributes\n",
    "        vals = []\n",
    "        for c in df_h.columns:\n",
    "            vals.append(\n",
    "                xr.DataArray(data=df_h[c],\n",
    "                             dims=['time'],\n",
    "                             coords={'time': df_h.index},\n",
    "                             attrs=ds[c].attrs)\n",
    "            )\n",
    "        dsouts.append(xr.Dataset(dict(zip(df_h.columns, vals)), attrs=ds.attrs))\n",
    "\n",
    "    if len(dsouts) == 1:\n",
    "        dsouts = dsouts[0]\n",
    "    return dsouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Default config and nc encoding\n",
    "Parse default config and cfmeta, and merge with user condfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.388328100Z",
     "start_time": "2024-01-29T14:31:29.594017100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_config(config: dict|None = None) -> dict:\n",
    "    \"\"\"Read default config and merge with input config\n",
    "    \"\"\"\n",
    "\n",
    "    fn_config = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_config.json\")\n",
    "    default_config = pyrutils.read_json(fn_config)\n",
    "    if config is None:\n",
    "        config = default_config\n",
    "    config = {**default_config, **config}\n",
    "\n",
    "    # add default files\n",
    "    cfiles = {\n",
    "        \"file_cfmeta\": \"share/pyrnet_cfmeta.json\",\n",
    "        \"file_calibration\": \"share/pyrnet_calibration.json\",\n",
    "        \"file_mapping\": \"share/pyrnet_station_map.json\",\n",
    "        \"file_gti_angles\": \"share/pyrnet_gti_angles.json\",\n",
    "        \"file_site\": \"share/pyrnet_sites.json\",\n",
    "    }\n",
    "    for fn in cfiles:\n",
    "        if config[fn] is None:\n",
    "            config[fn] =  pkg_res.resource_filename(\"pyrnet\", cfiles[fn])\n",
    "    return config\n",
    "\n",
    "def get_cfmeta(config: dict|None = None) -> dict:\n",
    "    \"\"\"Read global and variable attributes and encoding from cfmeta.json\n",
    "    \"\"\"\n",
    "    config= get_config(config)\n",
    "    # parse the json file\n",
    "    cfdict = pyrutils.read_json(config[\"file_cfmeta\"])\n",
    "    # get global attributes:\n",
    "    gattrs = cfdict['attributes']\n",
    "    # apply config\n",
    "    gattrs = {k:v.format_map(config) for k,v in gattrs.items()}\n",
    "    # get variable attributes\n",
    "    d = pyrutils.get_var_attrs(cfdict)\n",
    "    # split encoding attributes\n",
    "    vattrs, vencode = pyrutils.get_attrs_enc(d)\n",
    "    return gattrs ,vattrs, vencode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoding\n",
    "The encoding attributes are defined within the file PyrNet/share/pyrnet_cfmeta.json per default.\n",
    "\n",
    "The netcdf packing of data is realized via the two attributes *scale_factor* and *add_offset*:\n",
    "  ```unpacked_variable = scale_factor * packed_variable + add_offset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.458410500Z",
     "start_time": "2024-01-29T14:31:29.606060600Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attributes': {'title': 'TROPOS pyranometer network (PyrNet) observations',\n",
       "  'institution': 'Leibniz Institute for Tropospheric Research (TROPOS)',\n",
       "  'department': 'Remote Sensing of Atmospheric Processes',\n",
       "  'department_team': 'Clouds, Aerosol and Radiation',\n",
       "  'address': 'Permoser Str. 15, 04318 Leipzig, Germany',\n",
       "  'source': 'TROPOS pyranometer network (PyrNet)',\n",
       "  'contact_person': 'Andreas Macke and the clouds, aerosol and radiation team of the remote sensing department, mailto:andreas.macke@tropos.de',\n",
       "  'contributor_name': '{contributor_name}',\n",
       "  'contributor_role': '{contributor_role}',\n",
       "  'authors_software': 'Hartwig Deneke, Jonas Witthuhn, mailto:deneke@tropos.de',\n",
       "  'creator_name': '{creator_name}',\n",
       "  'project': '{project}',\n",
       "  'references': 'https://doi.org/10.5194/amt-9-1153-2016',\n",
       "  'standard_name_vocabulary': 'CF Standard Name Table v81',\n",
       "  'conventions': 'CF-1.10, ACDD-1.3',\n",
       "  'license': 'CC-BY-SA 3.0'},\n",
       " 'variables': {'ta': {'type': 'u2',\n",
       "   'attributes': {'units': 'K',\n",
       "    'long_name': 'air temperature',\n",
       "    'standard_name': 'air_temperature',\n",
       "    'scale_factor': 0.12903225806451613,\n",
       "    'add_offset': 253.15,\n",
       "    '_FillValue': 65535,\n",
       "    'valid_range': [0, 775],\n",
       "    'zlib': True}},\n",
       "  'rh': {'type': 'u2',\n",
       "   'attributes': {'units': '1',\n",
       "    'long_name': 'air relative humidity',\n",
       "    'standard_name': 'relative_humidity',\n",
       "    'scale_factor': 0.0012903225806451613,\n",
       "    'add_offset': 0.0,\n",
       "    'valid_range': [0, 775],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'battery_voltage': {'type': 'u2',\n",
       "   'attributes': {'units': 'V',\n",
       "    'standard_name': 'battery_voltage',\n",
       "    'scale_factor': 0.0064516129032258064,\n",
       "    'add_offset': 0.0,\n",
       "    'valid_range': [0, 1023],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'ghi': {'type': 'u2',\n",
       "   'attributes': {'units': 'V',\n",
       "    'long_name': 'downwelling shortwave flux',\n",
       "    'standard_name': 'downwelling_shortwave_flux_in_air',\n",
       "    'scale_factor': 1.075268817204301e-05,\n",
       "    'add_offset': 0.0,\n",
       "    'valid_range': [0, 1023],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'gti': {'type': 'u2',\n",
       "   'attributes': {'units': 'V',\n",
       "    'long_name': 'downwelling shortwave flux measured on secondary platform, might be tilted',\n",
       "    'standard_name': 'downwelling_shortwave_flux_in_air',\n",
       "    'scale_factor': 1.075268817204301e-05,\n",
       "    'add_offset': 0.0,\n",
       "    'valid_range': [0, 1023],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'station': {'type': 'u1',\n",
       "   'attributes': {'units': '-',\n",
       "    'long_name': 'PyrNet unit box number',\n",
       "    '_FillValue': 255,\n",
       "    'zlib': True}},\n",
       "  'szen': {'type': 'u2',\n",
       "   'attributes': {'standard_name': 'solar_zenith_angle',\n",
       "    'units': 'degree',\n",
       "    'scale_factor': 0.005,\n",
       "    'add_offset': 0.0,\n",
       "    'valid_range': [0, 36000],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'sazi': {'type': 'u2',\n",
       "   'attributes': {'standard_name': 'solar_azimuth_angle',\n",
       "    'units': 'degree',\n",
       "    'scale_factor': 0.01,\n",
       "    'add_offset': 0.0,\n",
       "    'valid_range': [0, 36000],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'esd': {'type': 'u2',\n",
       "   'attributes': {'standard_name': 'earth_sun_distance',\n",
       "    'units': 'AU',\n",
       "    'scale_factor': 1e-06,\n",
       "    'add_offset': 0.98,\n",
       "    'valid_range': [0, 40000],\n",
       "    '_FillValue': 65535,\n",
       "    'zlib': True}},\n",
       "  'ghi_qc': {'type': 'u1',\n",
       "   'attributes': {'standard_name': 'quality_flag',\n",
       "    'long_name': 'Maintenance quality control flags for main pyranometer',\n",
       "    'note': 'Soiling describes subjectively the coverage of the pyranometer dome with dirt. The level flag is problematic if the bubble of the spirit level touches the reference ring, and bad if it is outside.',\n",
       "    'valid_range': [0, 11],\n",
       "    'flag_masks': [3, 3, 3, 12, 12],\n",
       "    'flag_values': [1, 2, 3, 4, 8],\n",
       "    'flag_meanings': 'soiling_light soiling_moderate soiling_heavy level_problematic level_bad',\n",
       "    '_FillValue': 255,\n",
       "    'zlib': True}},\n",
       "  'gti_qc': {'type': 'u1',\n",
       "   'attributes': {'standard_name': 'quality_flag',\n",
       "    'long_name': 'Maintenance quality control flags for secondary pyranometer',\n",
       "    'note': 'Soiling describes subjectively the coverage of the pyranometer dome with dirt. The level flag is problematic if the bubble of the spirit level touches the reference ring, and bad if it is outside.',\n",
       "    'valid_range': [0, 11],\n",
       "    'flag_masks': [3, 3, 3, 12, 12],\n",
       "    'flag_values': [1, 2, 3, 4, 8],\n",
       "    'flag_meanings': 'soiling_light soiling_moderate soiling_heavy level_problematic level_bad',\n",
       "    '_FillValue': 255,\n",
       "    'zlib': True}},\n",
       "  'lat': {'type': 'u4',\n",
       "   'attributes': {'standard_name': 'latitude',\n",
       "    'units': 'degree_north',\n",
       "    'scale_factor': 1e-06,\n",
       "    'add_offset': -90,\n",
       "    'valid_range': [0, 180000000.0],\n",
       "    '_FillValue': 4294967295,\n",
       "    'zlib': True}},\n",
       "  'lon': {'type': 'u4',\n",
       "   'attributes': {'standard_name': 'longitude',\n",
       "    'units': 'degree_east',\n",
       "    'scale_factor': 1e-06,\n",
       "    'add_offset': -180,\n",
       "    'valid_range': [0, 360000000.0],\n",
       "    '_FillValue': 4294967295,\n",
       "    'zlib': True}},\n",
       "  'iadc': {'type': 'u4',\n",
       "   'attributes': {'standard_name': 'index',\n",
       "    'comment': 'index to map gps to adc records',\n",
       "    'units': '-',\n",
       "    '_FillValue': 4294967295,\n",
       "    'zlib': True}},\n",
       "  'adctime': {'type': 'u4',\n",
       "   'attributes': {'standard_name': 'time',\n",
       "    '_FillValue': 4294967295,\n",
       "    'zlib': True}}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "fn_cfjson = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_cfmeta.json\")\n",
    "pyrutils.read_json(fn_cfjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Irradiance values\n",
    "\n",
    "Irradiance values are stored as Voltage for later calibration. Assigning 1500Wm-2 as maximum measurable irradiance from the irradiance sensor. The maximum counts (*valid_max*) measured by the logger can be calculated by:\n",
    " * maximum counts = $\\mathrm{gain}*1500*\\mathrm{C_{max}}*1023/3.3$\n",
    "\n",
    "Most calibration factors are in the area of 7.5 uV/Wm-2, assuming $\\mathrm{C_{max}}=8$uV/Wm-2 for this estimate seems sufficient. All radiation sensors are amplified with a fixed gain of $\\mathrm{gain}=300$.\n",
    "\n",
    "Later (level 1c+) the scale factor will be instrument specific by adding the calibration from V to W m-2\n",
    "\n",
    "### Temperature and humidity\n",
    "\n",
    "\n",
    "Calibration coefficients for Temperature and Humidity DKRF-4000 series (discontinued) https://www.driesen-kern.de/downloads/produktlinie_feuchte.pdf are:\n",
    " * Temperature (T) range :-20-80 degC  from 0-5V\n",
    " * relative Humidity (rH) range: 0-100% from 0-5V\n",
    "\n",
    "As the logger ADC range is 0-3.3V with a 10bit resolution, the sensors are measured through a voltage splitting circuit. Therefore, the ADC counts have to be doubled.\n",
    " * Voltage U [V] = $2* 3.3/1023$ [counts]\n",
    " * T = $-20 + 100*(U/5)$ [degC] = $253.15 + 100*(U/5)$ [K]\n",
    " * rH = $(U/5)$  [%]  = $(U/5)$    [%]\n",
    "\n",
    "### Ancillary data\n",
    "\n",
    "* solar zenith angle (*szen*)\n",
    "    * valid range unpacked: (0,180) (deg)\n",
    "    * packing in u2 integer (unsigned 16bit)\n",
    "        * fill value = $2^{16} - 1$\n",
    "        * scale_factor = $180 /(2^{16}-2)$\n",
    "* solar azimuth angle (*sazi*)\n",
    "    * valid range unpacked: (0,360)\n",
    "    * packing in u2 integer (unsigned 16bit)\n",
    "        * fill_value = $2^{16} -1$\n",
    "        * scale_factor= $360 / (2^{16}-2)$\n",
    "* earth sun distance (*esd*)\n",
    "    * valid range unpacked: (0.98,1.02)\n",
    "    * packing in u2 integer (unsigned 16bit)\n",
    "        * fill_value = $2^{16} -1$\n",
    "        * scale_factor= $(1.02-0.98)/(2^{16}-2)$\n",
    "        * add_offset = 0.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.519231100Z",
     "start_time": "2024-01-29T14:31:29.661534Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta scale_factor: 0.12903225806451613\n",
      "ta add_offset: 253.15\n",
      "ta valid_range: [0, 775]\n",
      "\n",
      "rh scale_factor: 0.0012903225806451613\n",
      "rh add_offset: 0\n",
      "rh valid_range: [0, 775]\n",
      "\n",
      "battery scale_factor: 0.0064516129032258064\n",
      "battery add_offset: 0\n",
      "battery valid_range: [0, 992]\n",
      "\n",
      "radflux scale_factor: 1.075268817204301e-05\n",
      "radflux add_offset: 0\n",
      "radflux valid_range: [0, 1023]\n",
      "\n",
      "scale_factor szen (deg): 0.005\n",
      "add_offset szen: 0.0 \n",
      "valid_range szen: [0, 36000]\n",
      "\n",
      "scale_factor sazi (deg): 0.01\n",
      "add_offset sazi: 0.0 \n",
      "valid_range sazi: [0, 36000]\n",
      "\n",
      "scale_factor esd (AU): 1.0000000000000008e-06\n",
      "add_offset esd: 0.98 \n",
      "valid_range esd: [0, 40000]\n",
      "\n",
      "scale_factor lat (degree_north): 1e-06\n",
      "add_offset lat (degree_north): -90.\n",
      "valid_range lat: [0, 180_000_000]\n",
      "\n",
      "scale_factor lon (degree_east): 1e-06\n",
      "add_offset lon (degree_east): -180.\n",
      "valid_range lon: [0, 360_000_000]\n"
     ]
    }
   ],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "sensor_config = {\n",
    "    \"ta\": {\n",
    "        \"C\": 100./5., # K/V\n",
    "        \"offset\": 253.15, # K at 0V\n",
    "        \"gain\": 0.5, # measured over voltage splitter\n",
    "        \"Vmax\": 5, # V output at sensor\n",
    "    },\n",
    "    \"rh\": {\n",
    "        \"C\": 1./5., # 1/V\n",
    "        \"offset\": 0, # 0 at 0V\n",
    "        \"gain\": 0.5, # measured over voltage splitter\n",
    "        \"Vmax\": 5, # V output at sensor\n",
    "    },\n",
    "    \"battery\": {\n",
    "        \"C\": 1., # V(batt)/V\n",
    "        \"offset\": 0, # V at 0V\n",
    "        \"gain\": 0.5, # measured over voltage splitter\n",
    "        \"Vmax\": 6.4, # V output at sensor\n",
    "    },\n",
    "    \"radflux\": {\n",
    "        \"C\": 1., # Vsensor/Vmeasured\n",
    "        \"offset\": 0, # V at 0V\n",
    "        \"gain\": 300, # amplified\n",
    "        \"Vmax\": 1500*8*1e-6, # V output at sensor\n",
    "    },\n",
    "}\n",
    "ADCfac = 3.3/1023. # V/counts\n",
    "for var in sensor_config:\n",
    "    sconf = sensor_config[var]\n",
    "    print(f\"{var} scale_factor: {sconf['C']*ADCfac/sconf['gain']}\")\n",
    "    print(f\"{var} add_offset: {sconf['offset']}\")\n",
    "    print(f\"{var} valid_range: [0, {min((1023, int(sconf['Vmax']*sconf['gain']/ADCfac)))}]\")\n",
    "    print()\n",
    "\n",
    "# szen encoding\n",
    "print(f\"scale_factor szen (deg): {180./(36_000)}\")\n",
    "print(f\"add_offset szen: 0.0 \")\n",
    "print(f\"valid_range szen: [0, {36_000}]\")\n",
    "print()\n",
    "# sazi encoding\n",
    "print(f\"scale_factor sazi (deg): {360./(36_000)}\")\n",
    "print(f\"add_offset sazi: 0.0 \")\n",
    "print(f\"valid_range sazi: [0, {36_000}]\")\n",
    "print()\n",
    "# esd encoding\n",
    "print(f\"scale_factor esd (AU): {(1.02-0.98)/(40_000)}\")\n",
    "print(f\"add_offset esd: 0.98 \")\n",
    "print(f\"valid_range esd: [0, {40_000}]\")\n",
    "print()\n",
    "# lat encoding\n",
    "print(f\"scale_factor lat (degree_north): {(90.-(-90.))/180_000_000}\")\n",
    "print(f\"add_offset lat (degree_north): -90.\")\n",
    "print(f\"valid_range lat: [0, 180_000_000]\")\n",
    "print()\n",
    "# lon encoding\n",
    "print(f\"scale_factor lon (degree_east): {(180.-(-180.))/360_000_000}\")\n",
    "print(f\"add_offset lon (degree_east): -180.\")\n",
    "print(f\"valid_range lon: [0, 360_000_000]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.706898300Z",
     "start_time": "2024-01-29T14:31:29.676634800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def add_encoding(ds, vencode=None):\n",
    "    \"\"\"\n",
    "    Set valid_range attribute and encoding to every variable of the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xr.Dataset\n",
    "        Dataset of any processing level. The processing level will be\n",
    "        determined by the global attribute 'processing_level'.\n",
    "    vencode: dict or None\n",
    "        Dictionary of encoding attributes by variable name, will be merged with pyrnet default cfmeta. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        The input dataset but with encoding and valid_range attribute.\n",
    "    \"\"\"\n",
    "    # prepare netcdf encoding\n",
    "    _, vattrs_default, vencode_default = get_cfmeta()\n",
    "\n",
    "    # Add valid range temporary to encoding dict.\n",
    "    # As valid_range is not implemented in xarray encoding,\n",
    "    #  it has to be stored as a variable attribute later.\n",
    "    for k in vencode_default:\n",
    "        if \"valid_range\" not in vencode_default[k]:\n",
    "            continue\n",
    "        vencode_default = assoc_in(vencode_default,\n",
    "                                   [k,'valid_range'],\n",
    "                                   vattrs_default['valid_range'])\n",
    "\n",
    "    # merge input and default with preference on input\n",
    "    if vencode is None:\n",
    "        vencode = vencode_default\n",
    "    else:\n",
    "        a = vencode_default.copy()\n",
    "        b = vencode.copy()\n",
    "        vencode = {}\n",
    "        for k in set(a)-set(b):\n",
    "            vencode.update({k:a[k]})\n",
    "        for k in set(a)&set(b):\n",
    "            vencode.update({k: {**a[k],**b[k]}})\n",
    "        for k in set(b)-set(a):\n",
    "            vencode.update({k:b[k]})\n",
    "\n",
    "    # add encoding to Dataset\n",
    "    for k, v in vencode.items():\n",
    "        if k not in ds.keys():\n",
    "            continue\n",
    "        for ki in [key for key in ds if key.startswith(k)]:\n",
    "            ds[ki].encoding = v\n",
    "        if \"valid_range\" not in vencode[k]:\n",
    "            continue\n",
    "        # add valid_range to variable attributes\n",
    "        for ki in [key for key in ds if key.startswith(k)]:\n",
    "            ds[ki].attrs.update({\n",
    "                'valid_range': vencode[k]['valid_range']\n",
    "            })\n",
    "\n",
    "    # special treatment of time and flux variables\n",
    "    if ds.processing_level=='l1a':\n",
    "        ds[\"gpstime\"].encoding.update({\n",
    "            \"dtype\": 'f8',\n",
    "            \"units\": f\"seconds since {np.datetime_as_string(ds.gpstime.data[0], unit='D')}T00:00Z\",\n",
    "        })\n",
    "        ds[\"adctime\"].encoding.update({\n",
    "            \"units\": f\"milliseconds\",\n",
    "        })\n",
    "    elif ds.processing_level == 'l1b':\n",
    "        ds = stretch_resolution(ds)\n",
    "        # special treatment for flux variables\n",
    "        for k in ['ghi', 'gti']:\n",
    "            if k not in ds:\n",
    "                continue\n",
    "            # add encoding\n",
    "            dtype = ds[k].encoding['dtype']\n",
    "            int_limit = np.iinfo(dtype).max\n",
    "            valid_range = [0, int_limit - 1]\n",
    "            scale_factor = 1500. / float(int_limit - 1)\n",
    "            ds[k].encoding.update({\n",
    "                \"scale_factor\": scale_factor,\n",
    "                \"_FillValue\": int_limit,\n",
    "            })\n",
    "            ds[k].attrs.update({\n",
    "                \"units\": \"W m-2\",\n",
    "                \"valid_range\": valid_range\n",
    "            })\n",
    "\n",
    "        ds[\"time\"].encoding.update({\n",
    "            \"dtype\": 'f8',\n",
    "            \"units\": f\"seconds since {np.datetime_as_string(ds.time.data[0], unit='D')}T00:00Z\",\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"Dataset has no 'processing_level' attribute.\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## l1a\n",
    "Full resolution unprocessed, uncalibrated data. One data file corresponds to one maintenance period of one PyrNet station.\n",
    "\n",
    "Level *l1a* will be processed from the logger raw data with the following workflow:\n",
    "\n",
    "1. Parse raw logger file\n",
    "    * ```pyrnet.logger.read_records```\n",
    "1. Get maintenance logbook quality flags\n",
    "    * ```pyrnet.reports```\n",
    "1. Get metadata and encoding\n",
    "    * ```pyrnet_cfmeta_l1b.json```\n",
    "1. Make xarray Dataset\n",
    "1. Add variable and global attributes and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:31.878665500Z",
     "start_time": "2024-01-29T14:31:29.741412700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_l1a(\n",
    "        fname : str,\n",
    "        *,\n",
    "        station: int,\n",
    "        report: dict|pd.DataFrame|None,\n",
    "        date_of_measure : np.datetime64 = np.datetime64(\"now\"),\n",
    "        config: dict|None = None,\n",
    "        global_attrs: dict|None = None\n",
    ") -> xr.Dataset|None:\n",
    "    \"\"\"\n",
    "    Read logger raw file and parse it to xarray Dataset. Thereby, attributes and names are defined via cfmeta.json file and sun position values are calculated and added.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        Path and filename of the raw logger file.\n",
    "    station: int\n",
    "        PyrNet station box number.\n",
    "    report: dict\n",
    "        Parsed maintenance report, see reports.ipynb\n",
    "    bins: int\n",
    "        Number of desired bins per day. The default is 86400, which result in\n",
    "        mean values of 1 second steps per day. Maximum resolution is 86400000.\n",
    "    date_of_measure: float, datetime or datetime64\n",
    "        A rough date of measure  to account for GPS week rollover. If measured in 2019, day resolution is recommended, before 2019 annual resolution, 2020 onwards not required. If float, interpreted as Julian day from 2000-01-01T12:00. the default is np.datetime64(\"now\").\n",
    "    config: dict\n",
    "        Stores processing specific configuration.\n",
    "            * cfjson -> path to cfmeta.json, the default is \"../share/pyrnet_cfmeta.json\"\n",
    "            * stripminutes -> number of minutes to be stripped from the data at start and end,\n",
    "                the default is 5.\n",
    "    global_attrs: dict\n",
    "        Additional global attributes for the Dataset. (Overrides cfmeta.json attributes)\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Raw Logger data for one measurement periode.\n",
    "    \"\"\"\n",
    "    config = get_config(config)\n",
    "    gattrs, vattrs, vencode = get_cfmeta(config)\n",
    "\n",
    "    if global_attrs is not None:\n",
    "        gattrs.update(global_attrs)\n",
    "\n",
    "    date_of_measure = pyrutils.to_datetime64(date_of_measure)\n",
    "\n",
    "    # 1. Parse raw file\n",
    "    rec_adc, rec_gprmc = pyrlogger.read_records(fname=fname, date_of_measure=date_of_measure)\n",
    "\n",
    "    if type(rec_adc)==bool or len(rec_gprmc.time)<3:\n",
    "        logger.debug(\"Failed to load the data from the file, because of not enough stable GPS data, or file is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Get ADC time\n",
    "    adctime = pyrlogger.get_adc_time(rec_adc)\n",
    "\n",
    "    # ADC to Volts\n",
    "    # Drop time and internal battery sensor output (columns 0 and 1)\n",
    "    adc_volts = 3.3 * rec_adc[:,2:] / 1023.\n",
    "\n",
    "    # 2. Get Logbook maintenance quality flags\n",
    "    key = f\"{station:03d}\"\n",
    "    if report is None:\n",
    "        logger.warning(\"No report available!\")\n",
    "        report = {}\n",
    "    if isinstance(report, pd.DataFrame):\n",
    "        logger.info(f\"Parsing report at date {rec_gprmc.time[-1]}\")\n",
    "        report = pyrreports.parse_report(report,\n",
    "                                date_of_maintenance=rec_gprmc.time[-1])\n",
    "\n",
    "    if key not in report:\n",
    "        logger.warning(f\"No report for station {station} available.\")\n",
    "        warnings.warn(f\"No report for station {station} available.\")\n",
    "        qc_main = pyrreports.get_qcflag(4,3)\n",
    "        qc_extra = pyrreports.get_qcflag(4,3)\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_general\"], \"No maintenance report!\")\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_general\"], \"No maintenance report!\")\n",
    "    else:\n",
    "        qc_main = pyrreports.get_qcflag(\n",
    "            qc_clean=report[key]['clean'],\n",
    "            qc_level=report[key]['align']\n",
    "        )\n",
    "        qc_extra = pyrreports.get_qcflag(\n",
    "            qc_clean=report[key]['clean2'],\n",
    "            qc_level=report[key]['align2']\n",
    "        )\n",
    "        # add qc notes\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_general\"], report[key][\"note_general\"])\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_general\"], report[key][\"note_general\"])\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_clean\"], report[key][\"note_clean\"])\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_clean\"], report[key][\"note_clean2\"])\n",
    "        vattrs = assoc_in(vattrs, [\"ghi_qc\",\"note_level\"], report[key][\"note_align\"])\n",
    "        vattrs = assoc_in(vattrs, [\"gti_qc\",\"note_level\"], report[key][\"note_align2\"])\n",
    "\n",
    "    # 3. Add global meta data\n",
    "    now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "    gattrs.update({\n",
    "        'processing_level': 'l1a',\n",
    "        'product_version': pyrnet_version,\n",
    "        'history': f'{now.isoformat()}: Generated level l1a  by pyrnet version {pyrnet_version}; ',\n",
    "    })\n",
    "    # add site information\n",
    "    if config['sites'] is not None:\n",
    "        sites = pyrutils.read_json(config['file_site'])[config['sites']]\n",
    "        if key in sites:\n",
    "            gattrs.update({ \"site\" : sites[key]})\n",
    "\n",
    "    # add gti angles\n",
    "    # default horizontal\n",
    "    vattrs = assoc_in(vattrs, [\"gti\",\"hangle\"], 0.)\n",
    "    vattrs = assoc_in(vattrs, [\"gti\",\"vangle\"], 0.)\n",
    "    # update with angles from mapping file\n",
    "    if config['gti_angles'] is not None:\n",
    "        gti_angles = pyrutils.read_json(config['file_gti_angles'])[config['gti_angles']]\n",
    "        if key in gti_angles:\n",
    "            hangle = np.nan if gti_angles[key][0] is None else gti_angles[key][0]\n",
    "            vangle = np.nan if gti_angles[key][1] is None else gti_angles[key][1]\n",
    "            vattrs = assoc_in(vattrs, [\"gti\",\"hangle\"], hangle)\n",
    "            vattrs = assoc_in(vattrs, [\"gti\",\"vangle\"], vangle)\n",
    "\n",
    "    if adc_volts.shape[1]<5: # gti data is not available\n",
    "        adc_volts = np.concatenate((adc_volts,-1*np.ones(adc_volts.shape[0])[:,None]),axis=1)\n",
    "\n",
    "    # 8. Make xarray Dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            \"ghi\": ((\"adctime\",\"station\"), adc_volts[:,2][:,None] / 300.), # [V]\n",
    "            \"gti\": ((\"adctime\",\"station\"), adc_volts[:,4][:,None] / 300.), # [V]\n",
    "            \"ta\": ((\"adctime\",\"station\"), 253.15 + 20.*2.*adc_volts[:,0][:,None]), # [K]\n",
    "            \"rh\": ((\"adctime\",\"station\"), 0.2*2.*adc_volts[:,1][:,None]), # [-]\n",
    "            \"battery_voltage\": ((\"adctime\",\"station\"), 2.*adc_volts[:,3][:,None]), # [V]\n",
    "            \"lat\": ((\"gpstime\",\"station\"), rec_gprmc.lat[:,None]), # [degN]\n",
    "            \"lon\": ((\"gpstime\",\"station\"), rec_gprmc.lon[:,None]), # [degE]\n",
    "            \"ghi_qc\": (\"station\", [qc_main]),\n",
    "            \"gti_qc\": (\"station\", [qc_extra]),\n",
    "            \"iadc\": ((\"gpstime\", \"station\"), rec_gprmc.iadc[:,None])\n",
    "        },\n",
    "        coords={\n",
    "            \"adctime\": (\"adctime\", adctime.astype('timedelta64[ns]')),\n",
    "            \"gpstime\": (\"gpstime\", rec_gprmc.time.astype('datetime64[ns]')),\n",
    "            \"station\": (\"station\", [station]),\n",
    "        },\n",
    "        attrs=gattrs\n",
    "    )\n",
    "\n",
    "    # drop ocurance of douplicate gps values\n",
    "    ds = ds.drop_duplicates(\"gpstime\")\n",
    "\n",
    "    # add global coverage attributes\n",
    "    ds = update_coverage_meta(ds, timevar=\"gpstime\")\n",
    "\n",
    "    # add attributes to Dataset\n",
    "    for k,v in vattrs.items():\n",
    "        if k not in ds.keys():\n",
    "            continue\n",
    "        ds[k].attrs = v\n",
    "\n",
    "    # add encoding to Dataset\n",
    "    ds = add_encoding(ds, vencode)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:31:32.998418900Z",
     "start_time": "2024-01-29T14:31:29.827449400Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: b'/mnt/c/Users/witthuhn/Documents/Project/2023_SGP/PyrNet/example_data/to_l1a_output.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/file_manager.py:209\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/mnt/c/Users/witthuhn/Documents/Project/2023_SGP/PyrNet/example_data/to_l1a_output.nc',), 'a', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '91b22385-a23a-43e6-9210-d2db37ebdbdd']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# read logger file to xarray\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ds \u001b[38;5;241m=\u001b[39m to_l1a(\n\u001b[1;32m     14\u001b[0m     fname\u001b[38;5;241m=\u001b[39mfn_data,\n\u001b[1;32m     15\u001b[0m     station\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m# actually test data is from station 9, but test reports are for station 1 and 2 only\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     global_attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTESTNOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test note.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m ds\u001b[38;5;241m.\u001b[39mto_netcdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../example_data/to_l1a_output.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m ds\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/core/dataset.py:1903\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1900\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 1903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_netcdf(  \u001b[38;5;66;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;00m\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1905\u001b[0m     path,\n\u001b[1;32m   1906\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   1907\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m   1908\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m   1909\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m   1910\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   1911\u001b[0m     unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims,\n\u001b[1;32m   1912\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[1;32m   1913\u001b[0m     multifile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1914\u001b[0m     invalid_netcdf\u001b[38;5;241m=\u001b[39minvalid_netcdf,\n\u001b[1;32m   1915\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/api.py:1213\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrecognized option \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid_netcdf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for engine \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1212\u001b[0m         )\n\u001b[0;32m-> 1213\u001b[0m store \u001b[38;5;241m=\u001b[39m store_open(target, mode, \u001b[38;5;28mformat\u001b[39m, group, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unlimited_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1216\u001b[0m     unlimited_dims \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mencoding\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munlimited_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:376\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    370\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    371\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    373\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    374\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    375\u001b[0m )\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(manager, group\u001b[38;5;241m=\u001b[39mgroup, mode\u001b[38;5;241m=\u001b[39mmode, lock\u001b[38;5;241m=\u001b[39mlock, autoclose\u001b[38;5;241m=\u001b[39mautoclose)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:323\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:379\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    380\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/file_manager.py:197\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/miniconda3/envs/pyrnet/lib/python3.11/site-packages/xarray/backends/file_manager.py:215\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    213\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    214\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 215\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: b'/mnt/c/Users/witthuhn/Documents/Project/2023_SGP/PyrNet/example_data/to_l1a_output.nc'"
     ]
    }
   ],
   "source": [
    "#|dropout\n",
    "fn_report = \"../../example_data/results-survey224783.csv\"\n",
    "fn_data = \"../../example_data/Pyr9_000.bin\"\n",
    "\n",
    "fn_cfmeta = pkg_res.resource_filename(\"pyrnet\", \"share/pyrnet_cfmeta.json\")\n",
    "\n",
    "\n",
    "# parse report\n",
    "df_report = pyrreports.get_responses(fn=\"../../example_data/results-survey224783.csv\")\n",
    "report = pyrreports.parse_report(df_report,\n",
    "                              date_of_maintenance=np.datetime64(\"2023-05-08T12:00\"))\n",
    "# read logger file to xarray\n",
    "ds = to_l1a(\n",
    "    fname=fn_data,\n",
    "    station=1, # actually test data is from station 9, but test reports are for station 1 and 2 only\n",
    "    # bins=86400, # seconds resolution\n",
    "    report=report,\n",
    "    config={\"file_cfmeta\": fn_cfmeta, \"stripminutes\": 0},\n",
    "    global_attrs={\"TESTNOTE\": \"This is a test note.\"}\n",
    ")\n",
    "\n",
    "ds.to_netcdf(\"../../example_data/to_l1a_output.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.446820200Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "import netCDF4\n",
    "netCDF4.Dataset(\"../../example_data/to_l1a_output.nc\",'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "## l1b\n",
    "Resampled, calibrated data as daily files per station. Resampling is per default to one second, but can be configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.455844500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = \"../../example_data/to_l1a_output.nc\"\n",
    "config = {\n",
    "    \"l1bfreq\":\"1s\",\n",
    "    \"stripminutes\":5,\n",
    "    \"average_latlon\":True,\n",
    "}\n",
    "\n",
    "config = get_config(config)\n",
    "gattrs, vattrs, vencode = get_cfmeta(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The l1b data is processed from the l1b data with the following workflow:\n",
    "\n",
    "1. Read l1a netcdf\n",
    "2. Sync GPS to ADC time\n",
    "    * ```sync_adc_time```\n",
    "3. Make dataset with new time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.461858500Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 1. Load l1a data\n",
    "ds_l1a = xr.open_dataset(fname)\n",
    "# check correct file\n",
    "if ds_l1a.processing_level != \"l1a\":\n",
    "    raise ValueError(f\"{fname} is not a l1a file.\")\n",
    "\n",
    "\n",
    "# 2. Sync GPS to ADC time\n",
    "adctime = pyrlogger.sync_adc_time(\n",
    "    adctime = ds_l1a.adctime.values,\n",
    "    gpstime = ds_l1a.gpstime.values,\n",
    "    iadc = ds_l1a.iadc.squeeze().values.astype(int),\n",
    "    check_results= False\n",
    ")\n",
    "\n",
    "# 3. Create new dataset (l1b) with synced time\n",
    "\n",
    "ds_l1b = ds_l1a.drop_dims('gpstime')\n",
    "ds_l1b = ds_l1b.drop_vars(['ghi_qc','gti_qc']) # keep only time dependend variables\n",
    "ds_l1b = ds_l1b.assign({'time': ('adctime', adctime)})\n",
    "ds_l1b = ds_l1b.swap_dims({\"adctime\":\"time\"})\n",
    "ds_l1b = ds_l1b.drop_vars(\"adctime\")\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.468845300Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Drop first and last X minutes of data to avoid bad data due to maintenance\n",
    "\n",
    "```{note}\n",
    "config.json -> \"stripminutes\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.474863600Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 4. Drop first and last <stripminutes> minutes of data to avoid bad data due to maintenance\n",
    "\n",
    "# stripminutes = np.timedelta64(int(config['stripminutes']), 'm')\n",
    "# for now use some seconds for demonstration\n",
    "stripminutes = np.timedelta64(2, 's')\n",
    "tslice = slice(ds_l1b.time.values[0] + stripminutes,\n",
    "               ds_l1b.time.values[-1] - stripminutes)\n",
    "ds_l1b = ds_l1b.sel(time=tslice)\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Calibrate radiation flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.480863700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. rad flux calibration\n",
    "box = ds_l1b.station.values[0]\n",
    "boxnumber, serial, cfac = pyrnet.meta_lookup(\n",
    "    ds_l1b.time.values[0],\n",
    "    box=box,\n",
    "    cfile=config['file_calibration'],\n",
    "    mapfile=config['file_mapping'],\n",
    ")\n",
    "\n",
    "print(f\"Meta Lookup:\")\n",
    "print(f\">> Box={box}\")\n",
    "print(f\">> serial(s)={serial}\")\n",
    "print(f\"calibration factor(s)={cfac}\")\n",
    "\n",
    "\n",
    "# calibrate radiation flux with gain=300\n",
    "for i, radflx in enumerate(config['radflux_varname']):\n",
    "    if cfac[i] is None:\n",
    "        # drop if calibration/instrument don't exist (probably secondary pyranometer).\n",
    "        ds_l1b = ds_l1b.drop_vars([var for var in ds_l1b if radflx in var])\n",
    "        continue\n",
    "    ds_l1b[radflx].values = ds_l1b[radflx].values*1e6/cfac[i] # V -> W m-2\n",
    "    ds_l1b[radflx].attrs['units'] = \"W m-2\",\n",
    "    ds_l1b[radflx].attrs.update({\n",
    "        \"units\": \"W m-2\",\n",
    "        \"serial\": serial[i],\n",
    "        \"calibration_factor\": cfac[i]\n",
    "    })\n",
    "    # ds_l1b[radflx].encoding.update({\n",
    "    #     'scale_factor': ds_l1b[radflx].encoding['scale_factor']*1e6/cfac[i]\n",
    "    # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.489143500Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Apply appropriate binning to ADC values\n",
    "    * ```pyrnet.data.resample```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.497928600Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 6. resample to desired resolution\n",
    "# --- This method is replaced with the data.resample function ---\n",
    "# ds_l1b1 = pyrlogger.resample_mean(ds_l1b,freq=config['l1bfreq'])\n",
    "# ds_l1b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.505944400Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 6. resample to desired resolution\n",
    "# save station coordinate\n",
    "station_dim = {\"station\": ds_l1b[\"station\"].values}\n",
    "\n",
    "# resample on time dimension with specified methods\n",
    "methods = ['mean'] + config[\"l1b_resample_stats\"]\n",
    "res = resample(\n",
    "    ds_l1b.squeeze().drop_vars(\"station\"), # drop station coordinate and variable\n",
    "    freq=config['l1bfreq'],\n",
    "    methods=methods,\n",
    "    kwargs=dict(skipna=True)\n",
    ")\n",
    "\n",
    "# add standard names for new variables\n",
    "ds_l1b = res[0]\n",
    "for i, method in enumerate(methods[1:]):\n",
    "    for var in config[\"radflux_varname\"]:\n",
    "        ds_l1b[f\"{var}_{method}\"] = res[i+1][var]\n",
    "        ds_l1b[f\"{var}_{method}\"].attrs.update({\n",
    "            \"standard_name\": f\"{method}_\"+ds_l1b[f\"{var}_{method}\"].attrs[\"standard_name\"]\n",
    "        })\n",
    "\n",
    "# add station dimension back again\n",
    "ds_l1b = ds_l1b.expand_dims(station_dim, axis=-1)\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.512340900Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " * stretch valid_range to not lose resolution due to averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.520836800Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# stretch valid range to not lose resolution due to averaging\n",
    "# ds_l1b = stretch_resolution(ds_l1b)\n",
    "# ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.527331100Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. Interpolate GPS coordinates to bin time\n",
    "    * ```xarray.interp```\n",
    "    ```{note}\n",
    "    At this point the descision to whether store geocoordinates in full time resolution or as mean over the time interval is made.\n",
    "\n",
    "    This is configured in config.json -> \"average_latlon\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.534927500Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "\n",
    "# 7. Interpolate GPS coordinates to bin time\n",
    "ds_gps = ds_l1a.drop_dims(\"adctime\")\n",
    "ds_gps = ds_gps.drop_vars(['iadc'])\n",
    "\n",
    "\n",
    "# Decide whether geo coordinates should be averaged or not\n",
    "\n",
    "# if config['average_latlon']:\n",
    "ds_gps_avg = ds_gps.mean('gpstime',skipna=True)\n",
    "ds_l1b_avg = xr.merge((ds_l1b,ds_gps_avg))\n",
    "\n",
    "# else:\n",
    "ds_gps = ds_gps.interp(gpstime=ds_l1b.time,\n",
    "                       kwargs={\"bounds_error\":False, \"fill_value\":np.nan})\n",
    "ds_gps = ds_gps.drop_vars(\"gpstime\")\n",
    "\n",
    "ds_l1b = xr.merge((ds_l1b,ds_gps))\n",
    "\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.540862800Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. Calculate sun position\n",
    "    * ```trosat.sunpos```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.549829100Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# 8. Calc and add sun position\n",
    "\n",
    "szen, sazi = sp.sun_angles(\n",
    "    time=ds_l1b.time.values[:,None],\n",
    "    lat=ds_l1b.lat.values,\n",
    "    lon=ds_l1b.lon.values\n",
    ")\n",
    "\n",
    "\n",
    "szen_avg, sazi_avg = sp.sun_angles(\n",
    "    time=ds_l1b_avg.time.values[:,None],\n",
    "    lat=ds_l1b_avg.lat.values,\n",
    "    lon=ds_l1b_avg.lon.values\n",
    ")\n",
    "\n",
    "szen  = szen.squeeze()\n",
    "sazi = sazi.squeeze()\n",
    "szen_avg = szen_avg.squeeze()\n",
    "sazi_avg = sazi_avg.squeeze()\n",
    "\n",
    "\n",
    "esd = np.mean(sp.earth_sun_distance(ds_l1b.time.values))\n",
    "\n",
    "print('szen (avg latlon):', szen_avg)\n",
    "print('szen:', szen)\n",
    "print('sazi  (avg latlon):', sazi_avg)\n",
    "print('sazi:', sazi)\n",
    "print('Earth-Sun Distance:',esd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.555041900Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "ds_l1b = ds_l1b.assign(\n",
    "    {\n",
    "        \"szen\": ((\"time\", \"station\"), szen[:,None]),\n",
    "        \"sazi\": ((\"time\", \"station\"), sazi[:,None]),\n",
    "        \"esd\": (\"station\", [esd])\n",
    "    }\n",
    ")\n",
    "for key in ['szen', 'sazi','esd']:\n",
    "    ds_l1b[key].attrs.update(vattrs[key])\n",
    "    # ds_l1b[key].encoding.update(vencode[key])\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. Update variables and global attributes and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.559029800Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropcode\n",
    "#|dropout\n",
    "# add global coverage attributes\n",
    "ds_l1b = update_coverage_meta(ds_l1b, timevar=\"time\")\n",
    "\n",
    "ds_l1b.attrs[\"processing_level\"] = 'l1b'\n",
    "now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "ds_l1b.attrs[\"history\"] = ds_l1b.history + f\"{now.isoformat()}: Generated level l1b  by pyrnet version {pyrnet_version}; \"\n",
    "\n",
    "ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.564048700Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = ds_l1b.sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.568039100Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# ds_l1b.to_netcdf(\"../../testnb/to_l1b_output.nc\",\n",
    "#                  encoding={'time':{'dtype':'float64'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.571045400Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# dsp = xr.load_dataset(\"../../testnb/to_l1b_output.nc\").sel(time=\"2019-07-15\")\n",
    "# plt.figure()\n",
    "# plt.plot(dsp.time, dsp.ghi)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.574061900Z"
    },
    "collapsed": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "#|dropcode\n",
    "def to_l1b(\n",
    "        fname: str,\n",
    "        *,\n",
    "        config: dict | None = None,\n",
    "        global_attrs: dict | None = None,\n",
    "        check_adc_sync: bool = True\n",
    ") -> xr.Dataset|None:\n",
    "\n",
    "    config = get_config(config)\n",
    "    gattrs, vattrs, vencode = get_cfmeta(config)\n",
    "\n",
    "    if global_attrs is not None:\n",
    "        gattrs.update(global_attrs)\n",
    "\n",
    "    # 1. Load l1a data\n",
    "    ds_l1a = xr.open_dataset(fname)\n",
    "    # check correct file\n",
    "    if ds_l1a.processing_level != \"l1a\":\n",
    "        logger.warning(f\"{fname} is not a l1a file. Skip.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Sync GPS to ADC time\n",
    "    adctime = pyrlogger.sync_adc_time(\n",
    "        adctime = ds_l1a.adctime.values,\n",
    "        gpstime = ds_l1a.gpstime.values,\n",
    "        iadc = ds_l1a.iadc.squeeze().values.astype(int),\n",
    "        check_results = check_adc_sync\n",
    "    )\n",
    "    \n",
    "    if adctime is None:\n",
    "        logger.warning(f\"Could not fit GPS to ADC time for file {fname}. Skip.\")\n",
    "        return None\n",
    "\n",
    "    # 3. Create new dataset (l1b)\n",
    "    ds_l1b = ds_l1a.drop_dims('gpstime')\n",
    "    ds_l1b = ds_l1b.drop_vars(['ghi_qc','gti_qc']) # keep only time dependend variables\n",
    "    ds_l1b = ds_l1b.assign({'time': ('adctime', adctime)})\n",
    "    ds_l1b = ds_l1b.swap_dims({\"adctime\":\"time\"})\n",
    "    ds_l1b = ds_l1b.drop_vars(\"adctime\")\n",
    "\n",
    "    ds_l1b[\"time\"].encoding.update({\n",
    "        \"dtype\": 'float64',\n",
    "        \"units\": f\"seconds since {np.datetime_as_string(ds_l1b.time.data[0], unit='D')}T00:00Z\",\n",
    "    })\n",
    "    logger.info(f\"Dataset time coverage before strip: {ds_l1b.time.values[0]} - {ds_l1b.time.values[-1]}\")\n",
    "\n",
    "    # 4. Drop first and last <stripminutes> minutes of data to avoid bad data due to maintenance\n",
    "    stripminutes = np.timedelta64(int(config['stripminutes']), 'm')\n",
    "    if (ds_l1b.time.values[0] + 3*stripminutes) > ds_l1b.time.values[-1]:\n",
    "        logger.warning(f\"{fname} has not enough data. Skip.\")\n",
    "        return None\n",
    "\n",
    "    ds_l1b = ds_l1b.isel(time=ds_l1b.time>ds_l1b.time.values[0] + stripminutes)\n",
    "    ds_l1b = ds_l1b.isel(time=ds_l1b.time<ds_l1b.time.values[-1] - stripminutes)\n",
    "    if ds_l1b.time.size < 10:\n",
    "        logger.warning(f\"{fname} has not enough data, after strip. Skip.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    logger.info(f\"Dataset time coverage after strip: {ds_l1b.time.values[0]} - {ds_l1b.time.values[-1]}\")\n",
    "\n",
    "    # 5. rad flux calibration\n",
    "    box = ds_l1b.station.values[0]\n",
    "    boxnumber, serial, cfac = pyrnet.meta_lookup(\n",
    "        ds_l1b.time.values[0],\n",
    "        box=box,\n",
    "        cfile=config['file_calibration'],\n",
    "        mapfile=config['file_mapping'],\n",
    "    )\n",
    "    logger.info(f\"Meta Lookup:\")\n",
    "    logger.info(f\">> Box={box}\")\n",
    "    logger.info(f\">> serial(s)={serial}\")\n",
    "    logger.info(f\">> calibration factor(s)={cfac}\")\n",
    "\n",
    "\n",
    "    # calibrate radiation flux with gain=300\n",
    "    for i, radflx in enumerate(config['radflux_varname']):\n",
    "        if cfac[i] is None:\n",
    "            # drop if calibration/instrument don't exist (probably secondary pyranometer).\n",
    "            ds_l1b = ds_l1b.drop_vars([var for var in ds_l1b if radflx in var])\n",
    "            continue\n",
    "        ds_l1b[radflx].values = ds_l1b[radflx].values*1e6/(cfac[i]) # V -> W m-2\n",
    "        ds_l1b[radflx].attrs['units'] = \"W m-2\",\n",
    "        ds_l1b[radflx].attrs.update({\n",
    "            \"units\": \"W m-2\",\n",
    "            \"serial\": serial[i],\n",
    "            \"calibration_factor\": cfac[i]\n",
    "        })\n",
    "        # ds_l1b[radflx].encoding.update({\n",
    "        #     'scale_factor': ds_l1b[radflx].encoding['scale_factor']*1e6/(cfac[i])\n",
    "        # })\n",
    "\n",
    "\n",
    "    # 6. resample to desired resolution\n",
    "    # save station coordinate\n",
    "    station_dim = {\"station\": ds_l1b[\"station\"].values}\n",
    "    \n",
    "    # resample on time dimension with specified methods\n",
    "    methods = ['mean'] + config[\"l1b_resample_stats\"]\n",
    "    res = resample(\n",
    "        ds_l1b.squeeze().drop_vars(\"station\"), # drop station coordinate and variable\n",
    "        freq=config['l1bfreq'],\n",
    "        methods=methods,\n",
    "        kwargs=dict(skipna=True)\n",
    "    )\n",
    "    \n",
    "    # add standard names for new variables\n",
    "    ds_l1b = res[0]\n",
    "    for i, method in enumerate(methods[1:]):\n",
    "        for var in config[\"radflux_varname\"]:\n",
    "            ds_l1b[f\"{var}_{method}\"] = res[i+1][var]\n",
    "            ds_l1b[f\"{var}_{method}\"].attrs.update({\n",
    "                \"standard_name\": f\"{method}_\"+ds_l1b[f\"{var}_{method}\"].attrs[\"standard_name\"]\n",
    "            })\n",
    "    \n",
    "    # add station dimension back again\n",
    "    ds_l1b = ds_l1b.expand_dims(station_dim, axis=-1)\n",
    "    \n",
    "    # stretch valid range to not lose resolution due to averaging\n",
    "    # TODO: revise stretch resolution \n",
    "    ds_l1b = stretch_resolution(ds_l1b)\n",
    "\n",
    "    # 7. Interpolate GPS coordinates to l1b time\n",
    "    ds_gps = ds_l1a.drop_dims(\"adctime\")\n",
    "    ds_gps = ds_gps.drop_vars(['iadc'])\n",
    "\n",
    "    # Decide whether geo coordinates should be averaged or not\n",
    "    if config['average_latlon']:\n",
    "        ds_gps = ds_gps.mean('gpstime', skipna=True, keep_attrs=True)\n",
    "    else:\n",
    "        ds_gps = ds_gps.interp(gpstime=ds_l1b.time)\n",
    "        ds_gps = ds_gps.drop_vars(\"gpstime\")\n",
    "\n",
    "    ds_l1b = xr.merge((ds_l1b,ds_gps))\n",
    "\n",
    "    # 8. Calc and add sun position\n",
    "    szen, sazi = sp.sun_angles(\n",
    "        time=ds_l1b.time.values[:,None], # line up with coordinates to keep dependence on time only\n",
    "        lat=ds_l1b.lat.values,\n",
    "        lon=ds_l1b.lon.values\n",
    "    )\n",
    "    szen  = szen.squeeze()\n",
    "    sazi = sazi.squeeze()\n",
    "\n",
    "    esd = np.mean(sp.earth_sun_distance(ds_l1b.time.values))\n",
    "\n",
    "    ds_l1b = ds_l1b.assign(\n",
    "        {\n",
    "            \"szen\": ((\"time\", \"station\"), szen[:,None]),\n",
    "            \"sazi\": ((\"time\", \"station\"), sazi[:,None]),\n",
    "            \"esd\": (\"station\", [esd])\n",
    "        }\n",
    "    )\n",
    "    # update attributes and encoding\n",
    "    for key in ['szen', 'sazi','esd']:\n",
    "        ds_l1b[key].attrs.update(vattrs[key])\n",
    "        # ds_l1b[key].encoding.update(vencode[key])\n",
    "\n",
    "    # add global coverage attributes\n",
    "    ds_l1b = update_coverage_meta(ds_l1b, timevar=\"time\")\n",
    "    ds_l1b.attrs[\"processing_level\"] = 'l1b'\n",
    "    now = pd.to_datetime(np.datetime64(\"now\"))\n",
    "    ds_l1b.attrs[\"history\"] = ds_l1b.history + f\"{now.isoformat()}: Generated level l1b  by pyrnet version {pyrnet_version}; \"\n",
    "\n",
    "    # update encoding\n",
    "    ds_l1b = add_encoding(ds_l1b, vencode=vencode)\n",
    "\n",
    "    return ds_l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.578051400Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "fname = \"../../example_data/to_l1a_output.nc\"\n",
    "config = {\n",
    "    \"l1bfreq\":\"1s\",\n",
    "    \"stripminutes\":0,\n",
    "    \"average_latlon\":True,\n",
    "    \"l1b_resample_stats\": [\"min\", \"max\", \"std\"]\n",
    "}\n",
    "\n",
    "ds_l1b = to_l1b(fname=fname, config=config, check_adc_sync=False)\n",
    "ds_l1b.to_netcdf(\"../../example_data/to_l1b_output.nc\",\n",
    "                 encoding={'time':{'dtype':'float64'}}) # use float64 and not int64 for opendap2 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.582672100Z"
    },
    "collapsed": false,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|dropout\n",
    "import netCDF4\n",
    "netCDF4.Dataset(\"../../example_data/to_l1b_output.nc\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T14:31:32.586072Z"
    },
    "collapsed": false,
    "tags": [
     "remove-cell",
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Export module\n",
    "# Requires *nbdev* to export and update the *../lib/logger.py* module\n",
    "import nbdev.export\n",
    "import nbformat as nbf\n",
    "name = \"data\"\n",
    "\n",
    "# Export python module\n",
    "nbdev.export.nb_export( f\"{name}.ipynb\" ,f\"../../src/pyrnet\")\n",
    "\n",
    "# Export to docs\n",
    "ntbk = nbf.read(f\"{name}.ipynb\", nbf.NO_CONVERT)\n",
    "\n",
    "text_search_dict = {\n",
    "    \"#|hide\": \"remove-cell\",  # Remove the whole cell\n",
    "    \"#|dropcode\": \"hide-input\",  # Hide the input w/ a button to show\n",
    "    \"#|dropout\": \"hide-output\"  # Hide the output w/ a button to show\n",
    "}\n",
    "for cell in ntbk.cells:\n",
    "    cell_tags = cell.get('metadata', {}).get('tags', [])\n",
    "    for key, val in text_search_dict.items():\n",
    "            if key in cell['source']:\n",
    "                if val not in cell_tags:\n",
    "                    cell_tags.append(val)\n",
    "    if len(cell_tags) > 0:\n",
    "        cell['metadata']['tags'] = cell_tags\n",
    "    nbf.write(ntbk, f\"../../docs/source/nbs/{name}.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
