# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/pyrnet/data.ipynb.

# %% auto 0
__all__ = ['pyrnet_version', 'logger', 'get_fname', 'update_coverage_meta', 'stretch_resolution', 'to_netcdf', 'to_netcdf_l1b',
           'resample', 'get_config', 'get_sensor_config', 'get_cfmeta', 'calc_encoding', 'add_encoding', 'to_l1a',
           'to_l1b', 'merge_l1b']

# %% ../../nbs/pyrnet/data.ipynb 2
import os
import numpy as np
import pandas as pd
import xarray as xr
import logging
from toolz import assoc_in, merge_with
import pkg_resources as pkg_res
import warnings

from trosat import sunpos as sp
from trosat.cfconv import read_cfjson

import pyrnet
pyrnet_version = pyrnet.__version__
import pyrnet.pyrnet
import pyrnet.utils
import pyrnet.logger
import pyrnet.reports
import pyrnet.qcrad

# logging setup
logging.basicConfig(
    filename='pyrnet.log',
    encoding='utf-8',
    level=logging.DEBUG,
    format='%(asctime)s %(name)s %(levelname)s:%(message)s'
)
logger = logging.getLogger(__name__)

# %% ../../nbs/pyrnet/data.ipynb 6
def get_fname(ds, freq, period=None, timevar='time', sfx='nc', kind=None, station=None, config=None):
    config = get_config(config)
    if period is None:
        period = ds[timevar].values[-1] - ds[timevar].values[0]
        period = pd.to_timedelta(period).floor("s").isoformat()
    if kind is None:
        if ds.station.size==1:
            kind = 's'
            station = ds.station.values[0]
        else:
            kind = 'n'
            station = ds.station.size
    format_dict = dict(
        dt = pd.to_datetime(ds[timevar].values[0]),
        period = period,
        campaign = config["campaign"],
        kind = kind,
        station = int(station),
        level = ds.processing_level,
        freq = freq,
        collection = int(config["collection"]),
        sfx = sfx
    )
    return config["output"].format(**format_dict) 

# %% ../../nbs/pyrnet/data.ipynb 8
def update_coverage_meta(ds, timevar='time'):
    """Update global attributes related to geospatial and time coverage
    """
    duration = ds[timevar].values[-1] - ds[timevar].values[0]
    resolution = np.mean(np.diff(ds[timevar].values))
    now = pd.to_datetime(np.datetime64("now"))
    gattrs = {
        'date_created': now.isoformat(),
        'geospatial_lat_min': np.nanmin(ds.lat.values),
        'geospatial_lat_max': np.nanmax(ds.lat.values),
        'geospatial_lat_units': 'degN',
        'geospatial_lon_min': np.nanmin(ds.lon.values),
        'geospatial_lon_max': np.nanmax(ds.lon.values),
        'geospatial_lon_units': 'degE',
        'time_coverage_start': pd.to_datetime(ds[timevar].values[0]).isoformat(),
        'time_coverage_end': pd.to_datetime(ds[timevar].values[-1]).isoformat(),
        'time_coverage_duration': pd.to_timedelta(duration).isoformat(),
        'time_coverage_resolution': pd.to_timedelta(resolution).isoformat(),
    }
    ds.attrs.update(gattrs)
    return ds


# %% ../../nbs/pyrnet/data.ipynb 10
def stretch_resolution(ds: xr.Dataset) -> xr.Dataset:
    """ Stretch variable resolution to full integer size,
    to not lose resolution after averaging ADC count data."""
    for var in ds:
        if "scale_factor" not in ds[var].encoding:
            continue
        if "valid_range" not in ds[var].attrs:
            continue
        dtype = ds[var].encoding['dtype']
        valid_range = ds[var].valid_range
        int_limit = np.iinfo(dtype).max
        scale_factor = ds[var].encoding['scale_factor']
        scale_factor_mod = int((int_limit-1)/valid_range[1])
        ds[var].encoding.update({
            "scale_factor": scale_factor / scale_factor_mod,
            "_FillValue": int_limit,
        })
        ds[var].attrs.update({
            "valid_range": valid_range * scale_factor_mod
        })
    return ds

# %% ../../nbs/pyrnet/data.ipynb 12
def to_netcdf(ds, fname, timevar="time"):
    """xarray to netcdf, but merge if exist
    """
    # save to netCDF4
    ds = update_coverage_meta(ds, timevar=timevar)
    ds.to_netcdf(fname,
                 encoding={timevar:{'dtype':'float64'}}) # for OpenDAP 2 compatibility
#|export
def to_netcdf_l1b(ds, fname, freq='1s', timevar="time"):
    """xarray to netcdf, but merge if exist
    """
    # merge if necessary
    if isinstance(ds, xr.Dataset):
        dslist = [ds]
    else:
        dslist = ds
    
    if os.path.exists(fname):
        ds1 = xr.load_dataset(fname)
        dslist.append(ds1)
        
    ds = merge_l1b(dslist, freq=freq, timevar=timevar)
    # save to netCDF4
    ds = update_coverage_meta(ds, timevar=timevar)
    
    if os.path.exists(fname): 
        os.remove(fname)
    ds.to_netcdf(fname,
                 encoding={timevar:{'dtype':'float64'}}) # for OpenDAP 2 compatibility

# %% ../../nbs/pyrnet/data.ipynb 14
def resample(ds, freq, methods='mean', kwargs={}):
    """ Resample xarray dataset using pandas for speed.
    https://github.com/pydata/xarray/issues/4498#issuecomment-706688398
    """
    if isinstance(methods,str):
        methods = [methods]

    dsr = ds.to_dataframe().resample(freq)
    dsouts = []
    for method in methods:
        # what we want (quickly), but in Pandas form
        with warnings.catch_warnings():
            warnings.simplefilter(action='ignore', category=FutureWarning)
            df_h = dsr.apply(method)
        # rebuild xarray dataset with attributes
        vals = []
        for c in df_h.columns:
            vals.append(
                xr.DataArray(data=df_h[c],
                             dims=['time'],
                             coords={'time': df_h.index},
                             attrs=ds[c].attrs)
            )
        dsouts.append(xr.Dataset(dict(zip(df_h.columns, vals)), attrs=ds.attrs))

    if len(dsouts) == 1:
        dsouts = dsouts[0]
    return dsouts

# %% ../../nbs/pyrnet/data.ipynb 16
def get_config(config: dict|None = None) -> dict:
    """Read default config and merge with input config
    """

    fn_config = pkg_res.resource_filename("pyrnet", "share/pyrnet_config.json")
    default_config = pyrnet.utils.read_json(fn_config)
    if config is None:
        config = default_config
    config = {**default_config, **config}

    # add default files
    cfiles = {
        "file_cfmeta": "share/pyrnet_cfmeta.json",
        "file_calibration": "share/pyrnet_calibration.json",
        "file_mapping": "share/pyrnet_station_map.json",
        "file_gti_angles": "share/pyrnet_gti_angles.json",
        "file_site": "share/pyrnet_sites.json",
    }
    for fn in cfiles:
        if config[fn] is None:
            config[fn] =  pkg_res.resource_filename("pyrnet", cfiles[fn])
    return config

def get_sensor_config(sconfig: dict|None = None) -> dict:
    """ Read the sensor configuration from the default json file and merge if needed. 
    """
    fn_config = pkg_res.resource_filename("pyrnet", "share/pyrnet_sensor_config.json")
    default_config = pyrnet.utils.read_json(fn_config)
    if sconfig is None:
        sconfig = default_config
    sconfig = {**default_config, **sconfig}
    return sconfig

def get_cfmeta(config: dict|None = None) -> dict:
    """Read global and variable attributes and encoding from cfmeta.json
    """
    config= get_config(config)
    # parse the json file
    cfdict = read_cfjson(config["file_cfmeta"])
    # get global attributes:
    gattrs = cfdict['attributes']
    # apply config
    gattrs = {k:v.format_map(config) for k,v in gattrs.items()}
    # get variable attributes
    d = pyrnet.utils.get_var_attrs(cfdict)
    # split encoding attributes
    vattrs, vencode = pyrnet.utils.get_attrs_enc(d)
    return gattrs, vattrs, vencode

# %% ../../nbs/pyrnet/data.ipynb 18
def calc_encoding(sconfig:dict, ADCV=3.3, ADCbits=10) -> dict:
    ADCfac = ADCV / (2**ADCbits-1) # Last bit is reserved 
    sencoding = {}
    for k, v in sconfig.items():
        sencoding.update(
            {k: dict(
                units=v['units'],
                scale_factor=v['C']*ADCfac/v['gain'],
                add_offset=v['offset'],
                valid_range= [0, min(((2**ADCbits-1), int(v['Vmax']*v['gain']/ADCfac)))]
            )}
        )
    return sencoding

# %% ../../nbs/pyrnet/data.ipynb 24
def add_encoding(ds, vencode=None):
    """
    Set valid_range attribute and encoding to every variable of the dataset.

    Parameters
    ----------
    ds: xr.Dataset
        Dataset of any processing level. The processing level will be
        determined by the global attribute 'processing_level'.
    vencode: dict or None
        Dictionary of encoding attributes by variable name, will be merged with pyrnet default cfmeta. The default is None.

    Returns
    -------
    xr.Dataset
        The input dataset but with encoding and valid_range attribute.
    """
    # prepare netcdf encoding
    _, vattrs_default, vencode_default = get_cfmeta()

    # Add valid range temporary to encoding dict.
    # As valid_range is not implemented in xarray encoding,
    #  it has to be stored as a variable attribute later.
    for k in vencode_default:
        if "valid_range" in vencode_default[k]:
            continue
        if "valid_range" not in vattrs_default[k]:
            continue
        vencode_default = assoc_in(vencode_default,
                                   [k,'valid_range'],
                                   vattrs_default[k]['valid_range'])
        
    # merge input and default with preference on input
    if vencode is None:
        vencode = vencode_default
    else:
        a = vencode_default.copy()
        b = vencode.copy()
        vencode = {}
        for k in set(a)-set(b):
            vencode.update({k:a[k]})
        for k in set(a)&set(b):
            vencode.update({k: {**a[k],**b[k]}})
        for k in set(b)-set(a):
            vencode.update({k:b[k]})

    # add encoding to Dataset
    for k, v in vencode.items():
        for ki in [key for key in ds if key.startswith(k)]:
            ds[ki].encoding.update(v)
        if "valid_range" not in vencode[k]:
            continue
        # add valid_range to variable attributes
        for ki in [key for key in ds if key.startswith(k)]:
            ds[ki].attrs.update({
                'valid_range': vencode[k]['valid_range']
            })
            
    # add encoding to coords
    if ds.processing_level=='l1a':
        ds["gpstime"].encoding.update({
            **vencode["time"],
            "units": f"seconds since {np.datetime_as_string(ds.gpstime.data[0], unit='D')}T00:00Z",
        })
        ds["maintenancetime"].encoding.update({
            **vencode["time"],
            "units": f"seconds since {np.datetime_as_string(ds.maintenancetime.data[0], unit='D')}T00:00Z",
        })
        ds["adctime"].encoding.update({
            **vencode["adctime"],
            "units": "milliseconds"
        })
        ds["station"].encoding.update({
            **vencode["station"]
        })
    elif ds.processing_level == 'l1b':
        ds["time"].encoding.update({
            **vencode["time"],
            "units": f"seconds since {np.datetime_as_string(ds.time.data[0], unit='D')}T00:00Z",
        })
        ds["maintenancetime"].encoding.update({
            **vencode["time"],
            "units": f"seconds since {np.datetime_as_string(ds.maintenancetime.data[0], unit='D')}T00:00Z",
        })
    else:
        raise ValueError("Dataset has no 'processing_level' attribute.")
    return ds

# %% ../../nbs/pyrnet/data.ipynb 29
def to_l1a(
        fname : str,
        *,
        station: int,
        report: dict|pd.DataFrame|None,
        date_of_measure : np.datetime64 = np.datetime64("now"),
        config: dict|None = None,
        sconfig: dict|None = None,
        global_attrs: dict|None = None
) -> xr.Dataset|None:
    """
    Read logger raw file and parse it to xarray Dataset. Thereby, attributes and names are defined via cfmeta.json file and sun position values are calculated and added.

    Parameters
    ----------
    fname: str
        Path and filename of the raw logger file.
    station: int
        PyrNet station box number.
    report: dict
        Parsed maintenance report, see reports.ipynb
    bins: int
        Number of desired bins per day. The default is 86400, which result in
        mean values of 1 second steps per day. Maximum resolution is 86400000.
    date_of_measure: float, datetime or datetime64
        A rough date of measure  to account for GPS week rollover. If measured in 2019, day resolution is recommended, before 2019 annual resolution, 2020 onwards not required. If float, interpreted as Julian day from 2000-01-01T12:00. the default is np.datetime64("now").
    config: dict
        Stores processing specific configuration.
            * cfjson -> path to cfmeta.json, the default is "../share/pyrnet_cfmeta.json"
            * stripminutes -> number of minutes to be stripped from the data at start and end,
                the default is 5.
    sconfig: dict
        Config for ADC and amplifier for each sensor. The default is "../share/pyrnet_sensor_config.json"
    global_attrs: dict
        Additional global attributes for the Dataset. (Overrides cfmeta.json attributes)
    Returns
    -------
    xarray.Dataset
        Raw Logger data for one measurement periode.
    """
    ADCV = 3.3
    ADCbits = 10
    
    # load and merge  default config
    config = get_config(config)
    gattrs, vattrs, vencode = get_cfmeta(config)
    
    # update encoding with sensor config for l1a
    sconfig = get_sensor_config(sconfig)
    sencoding = calc_encoding(sconfig, ADCV=ADCV, ADCbits=ADCbits)
    # update encoding with sensor config for l1a
    for var, enc in sencoding.items():
        for k, v in enc.items():
            if k in ["units"]:
                vattrs = assoc_in(vattrs, [var, k], v)
            else:
                vencode = assoc_in(vencode, [var, k], v)
                
    # update additional global attributes
    if global_attrs is not None:
        gattrs.update(global_attrs)

    date_of_measure = pyrnet.utils.to_datetime64(date_of_measure)

    # 1. Parse raw file
    rec_adc, rec_gprmc = pyrnet.logger.read_records(fname=fname, date_of_measure=date_of_measure)

    if type(rec_adc)==bool or len(rec_gprmc.time)<3:
        logger.debug("Failed to load the data from the file, because of not enough stable GPS data, or file is empty.")
        return None

    # Get ADC time
    adctime = pyrnet.logger.get_adc_time(rec_adc)

    # ADC to Volts
    # Drop time and internal battery sensor output (columns 0 and 1)
    adc_volts = ADCV * rec_adc[:,2:] / float(2**ADCbits - 1)

    # 2. Get Logbook maintenance quality flags
    key = f"{station:03d}"
    if report is None:
        logger.warning("No report available!")
        report = {}
    if isinstance(report, pd.DataFrame):
        logger.info(f"Parsing report at date {rec_gprmc.time[-1]}")
        report = pyrnet.reports.parse_report(
            report,
            date_of_maintenance=rec_gprmc.time[-1],
            stations=station
        )

    if key not in report:
        logger.warning(f"No report for station {station} available.")
        warnings.warn(f"No report for station {station} available.")
        qc_main = pyrnet.reports.get_qcflag(4,3)
        qc_extra = pyrnet.reports.get_qcflag(4,3)
        vattrs = assoc_in(vattrs, ["maintenance_flag_ghi","note_general"], "No maintenance report!")
        vattrs = assoc_in(vattrs, ["maintenance_flag_gti","note_general"], "No maintenance report!")
        vattrs = assoc_in(vattrs, ["maintenance_flag_ghi","note_clean"], "")
        vattrs = assoc_in(vattrs, ["maintenance_flag_gti","note_clean"], "")
        vattrs = assoc_in(vattrs, ["maintenance_flag_ghi","note_level"], "")
        vattrs = assoc_in(vattrs, ["maintenance_flag_gti","note_level"], "")
        maintenancetime = np.array([rec_gprmc.time.astype('datetime64[ns]')[-1]])
    else:
        qc_main = pyrnet.reports.get_qcflag(
            qc_clean=report[key]['clean'],
            qc_level=report[key]['align']
        )
        qc_extra = pyrnet.reports.get_qcflag(
            qc_clean=report[key]['clean2'],
            qc_level=report[key]['align2']
        )
        maintenancetime = np.array([pyrnet.utils.to_datetime64(report[key]["maintenancetime"])]) 
        # add qc notes
        vattrs = assoc_in(vattrs, ["maintenance_flag_ghi","note_general"], report[key]["note_general"])
        vattrs = assoc_in(vattrs, ["maintenance_flag_gti","note_general"], report[key]["note_general"])
        vattrs = assoc_in(vattrs, ["maintenance_flag_ghi","note_clean"], report[key]["note_clean"])
        vattrs = assoc_in(vattrs, ["maintenance_flag_gti","note_clean"], report[key]["note_clean2"])
        vattrs = assoc_in(vattrs, ["maintenance_flag_ghi","note_level"], report[key]["note_align"])
        vattrs = assoc_in(vattrs, ["maintenance_flag_gti","note_level"], report[key]["note_align2"])
    qc_main = np.ubyte(qc_main)
    qc_extra = np.ubyte(qc_extra)
    
    vattrs = assoc_in(vattrs, ["ghi","ancillary_variables"], "maintenance_flag_ghi")
    vattrs = assoc_in(vattrs, ["gti","ancillary_variables"], "maintenance_flag_gti")

    # 3. Add global meta data
    now = pd.to_datetime(np.datetime64("now"))
    gattrs.update({
        'processing_level': 'l1a',
        'product_version': pyrnet_version,
        'history': f'{now.isoformat()}: Generated level l1a  by pyrnet version {pyrnet_version}; ',
    })
    # add site information
    if config['sites'] is not None:
        sites = pyrnet.utils.read_json(config['file_site'])[config['sites']]
        if key in sites:
            gattrs.update({ "site" : sites[key]})

    # add gti angles
    # default horizontal
    vattrs = assoc_in(vattrs, ["gti","hangle"], 0.)
    vattrs = assoc_in(vattrs, ["gti","vangle"], 0.)
    # update with angles from mapping file
    if config['gti_angles'] is not None:
        gti_angles = pyrnet.utils.read_json(config['file_gti_angles'])[config['gti_angles']]
        if key in gti_angles:
            hangle = np.nan if gti_angles[key][0] is None else gti_angles[key][0]
            vangle = np.nan if gti_angles[key][1] is None else gti_angles[key][1]
            vattrs = assoc_in(vattrs, ["gti","hangle"], hangle)
            vattrs = assoc_in(vattrs, ["gti","vangle"], vangle)

    if adc_volts.shape[1]<5: # gti data is not available
        adc_volts = np.concatenate((adc_volts,-1*np.ones(adc_volts.shape[0])[:,None]),axis=1)

    

    # 8. Make xarray Dataset
    values = {}
    for k, v in sconfig.items():
        offset = sconfig[k]["offset"]
        gain = sconfig[k]["gain"]
        C = sconfig[k]["C"]
        iadc = sconfig[k]["iadc"]
        volts = adc_volts[:,iadc][:,None]
        values.update(
            {k: offset + C*volts/gain}
        )
    
    # Remove rh and temperature error if input voltage to low
    # Allow only sensor values of voltage V < (V(battery)-2),
    # as input voltage needs to be at least 2V larger then sensor output
    for k in ["rh","ta"]:
        iadc = sconfig[k]["iadc"]
        volts_sensor = (values[k][:,0]-sconfig[k]["offset"])/sconfig[k]["C"] # no gain, as we need voltage at sensor
        mask = volts_sensor > (values["battery_voltage"][:,0] - 2.) 
        values[k][mask,0] = np.nan 
    
    ds = xr.Dataset(
        data_vars={
            "ghi": (("adctime","station"), values["ghi"]), # [V]
            "gti": (("adctime","station"), values["gti"]), # [V]
            "ta": (("adctime","station"), values["ta"]), # [K]
            "rh": (("adctime","station"), values["rh"]), # [-]
            "battery_voltage": (("adctime","station"), values["battery_voltage"]), # [V]
            "lat": (("gpstime","station"), rec_gprmc.lat[:,None]), # [degN]
            "lon": (("gpstime","station"), rec_gprmc.lon[:,None]), # [degE]
            "maintenance_flag_ghi": (("maintenancetime","station"), [[qc_main]]),
            "maintenance_flag_gti": (("maintenancetime","station"), [[qc_extra]]),
            "iadc": (("gpstime", "station"), rec_gprmc.iadc[:,None])
        },
        coords={
            "adctime": ("adctime", adctime.astype('timedelta64[ns]')),
            "gpstime": ("gpstime", rec_gprmc.time.astype('datetime64[ns]')),
            "maintenancetime": ("maintenancetime", maintenancetime),
            "station": ("station", [np.ubyte(station)]),
        },
        attrs=gattrs
    )

    # drop ocurance of douplicate gps values
    ds = ds.drop_duplicates("gpstime")

    # add global coverage attributes
    ds = update_coverage_meta(ds, timevar="gpstime")

    # add attributes to Dataset
    for k,v in vattrs.items():
        for key in [key for key in ds if key.startswith(k)]:
            ds[key].attrs.update(v)

    # add encoding to Dataset
    ds = add_encoding(ds, vencode)

    return ds

# %% ../../nbs/pyrnet/data.ipynb 62
def to_l1b(
        fname: str,
        *,
        config: dict | None = None,
        global_attrs: dict | None = None,
        check_adc_sync: bool = True
) -> xr.Dataset|None:

    config = get_config(config)
    gattrs, vattrs, vencode = get_cfmeta(config)

    if global_attrs is not None:
        gattrs.update(global_attrs)

    ######################################################################################
    ## Load l1a data
    ds_l1a = xr.open_dataset(fname)
    # check correct file
    if ds_l1a.processing_level != "l1a":
        logger.warning(f"{fname} is not a l1a file. Skip.")
        return None

    ######################################################################################
    ## Sync GPS to ADC time
    adctime = pyrnet.logger.sync_adc_time(
        adctime = ds_l1a.adctime.values,
        gpstime = ds_l1a.gpstime.values,
        iadc = ds_l1a.iadc.squeeze().values.astype(int),
        check_results = check_adc_sync
    )
    
    if adctime is None:
        logger.warning(f"Could not fit GPS to ADC time for file {fname}. Skip.")
        return None

    ######################################################################################
    ## Create new dataset (l1b)
    ds_l1b = ds_l1a.drop_dims('gpstime')
    ds_l1b = ds_l1b.drop_vars(['maintenance_flag_ghi','maintenance_flag_gti']) # keep only time dependent variables
    ds_l1b = ds_l1b.assign({'time': ('adctime', adctime)})
    ds_l1b = ds_l1b.swap_dims({"adctime":"time"})
    ds_l1b = ds_l1b.drop_vars("adctime")

    ds_l1b["time"].encoding.update({
        "dtype": 'float64',
        "units": f"seconds since {np.datetime_as_string(ds_l1b.time.data[0], unit='D')}T00:00Z",
    })
    logger.info(f"Dataset time coverage before strip: {ds_l1b.time.values[0]} - {ds_l1b.time.values[-1]}")

    ######################################################################################
    ## Drop first and last <stripminutes> minutes of data to avoid bad data due to maintenance
    stripminutes = np.timedelta64(int(config['stripminutes']), 'm')
    if (ds_l1b.time.values[0] + 3*stripminutes) > ds_l1b.time.values[-1]:
        logger.warning(f"{fname} has not enough data. Skip.")
        return None

    ds_l1b = ds_l1b.isel(time=ds_l1b.time>ds_l1b.time.values[0] + stripminutes)
    ds_l1b = ds_l1b.isel(time=ds_l1b.time<ds_l1b.time.values[-1] - stripminutes)
    if ds_l1b.time.size < 10:
        logger.warning(f"{fname} has not enough data, after strip. Skip.")
        return None


    logger.info(f"Dataset time coverage after strip: {ds_l1b.time.values[0]} - {ds_l1b.time.values[-1]}")

    #####################################################################################
    ## resample to desired resolution
    # save station coordinate
    station_dim = {"station": ds_l1b["station"].values}
    station_attrs = ds_l1b["station"].attrs

    # resample on time dimension with specified methods
    methods = ['mean'] + config["l1b_resample_stats"]
    res = resample(
        ds_l1b.squeeze().drop_vars("station"), # drop station coordinate and variable
        freq=config['l1bfreq'],
        methods=methods,
        kwargs=dict(skipna=True)
    )
    
    # add standard names for new variables
    # apply for variables if both in config['radflux_varname'] and ds_l1b.keys()
    radflux_vars = list(set(config['radflux_varname'])&set(ds_l1b.keys()))
    ds_l1b = res[0]
    for i, method in enumerate(methods[1:]):
        for var in radflux_vars:
            ds_l1b[f"{var}_{method}"] = res[i+1][var]
            ds_l1b[f"{var}_{method}"].attrs.update({
                "standard_name": f"{method}_"+ds_l1b[f"{var}_{method}"].attrs["standard_name"]
            })
    
    # add station dimension back again
    ds_l1b = ds_l1b.expand_dims(station_dim, axis=-1)
    ds_l1b["station"].attrs.update(station_attrs)
    
    # add maintenancetime coord
    ds_l1b = ds_l1b.assign_coords({"maintenancetime":ds_l1a.maintenancetime})
    
    ######################################################################################
    ## Interpolate GPS coordinates to l1b time
    ds_gps = ds_l1a.drop_dims("adctime")
    ds_gps = ds_gps.drop_vars(['iadc'])

    # Decide whether geo coordinates should be averaged or not
    if config['average_latlon']:
        ds_gps = ds_gps.mean('gpstime', skipna=True, keep_attrs=True)
    else:
        ds_gps = ds_gps.interp(gpstime=ds_l1b.time)
        ds_gps = ds_gps.drop_vars("gpstime")

    ds_l1b = xr.merge((ds_l1b,ds_gps))

    ######################################################################################
    ## Calc and add sun position
    szen, sazi = sp.sun_angles(
        time=ds_l1b.time.values[:,None], # line up with coordinates to keep dependence on time only
        lat=ds_l1b.lat.values,
        lon=ds_l1b.lon.values
    )
    szen  = szen.squeeze()
    sazi = sazi.squeeze()

    esd = np.mean(sp.earth_sun_distance(ds_l1b.time.values))

    ds_l1b = ds_l1b.assign(
        {
            "szen": (("time", "station"), szen[:,None]),
            "sazi": (("time", "station"), sazi[:,None]),
            "esd": ("station", [esd])
        }
    )
    # update attributes and encoding
    for key in ['szen', 'sazi','esd']:
        ds_l1b[key].attrs.update(vattrs[key])

    ######################################################################################
    ## rad flux calibration
    box = ds_l1b.station.values[0]
    boxnumber, serial, cfac, CCcoef = pyrnet.pyrnet.meta_lookup(
        ds_l1b.time.values[0],
        box=box,
        cfile=config['file_calibration'],
        mapfile=config['file_mapping'],
    )
    logger.info(f"Meta Lookup:")
    logger.info(f">> Box={box}")
    logger.info(f">> serial(s)={serial}")
    logger.info(f">> calibration factor(s)={cfac}")

    mu0 = np.cos(np.deg2rad(ds_l1b.szen.values))
    
    # calibrate radiation flux with gain=300
    for i, radflx in enumerate(config['radflux_varname']):
        # all radflux related variables (including <radflux>_<resamplemethod> variables)
        radflx_vars = [var for var in ds_l1b if var.startswith(radflx)]
        if cfac[i] is None:
            # drop if calibration/instrument don't exist (probably secondary pyranometer).
            ds_l1b = ds_l1b.drop_vars(radflx_vars)
            continue
        
        # calc apparent zenith angle if possible
        mua = mu0.copy()
        if "vangle" in ds_l1b[radflx].attrs:
            vangle = pyrnet.utils.make_iter(ds_l1b[radflx].attrs["vangle"])
            hangle = pyrnet.utils.make_iter(ds_l1b[radflx].attrs["hangle"])
            mua = pyrnet.utils.calc_apparent_coszen(
                pitch=vangle,
                yaw=hangle,
                zen=ds_l1b.szen.values,
                azi=ds_l1b.sazi.values
            )
        mua[mua<=0] = np.nan
        mask_mua = ~np.isnan(mua)
        Ca = 1e6/cfac[i]
        Cc = np.polynomial.polynomial.polyval(mua, c=CCcoef)
        Cmu = mu0/mua
        # apply to all variables
        for var in radflx_vars:
            calib_func = "flux (W m-2) = flux (V) * Cabsolute (W m-2 V-1)"
            C = np.ones(mu0.shape)*Ca
            if radflx == "gti":
                C[mask_mua] *= Cc[mask_mua]
                calib_func += "" if np.all(np.isnan(mua)) else " * Ccoscorr(mua)"
            else:
                C[mask_mua] *= Cc[mask_mua] * Cmu[mask_mua]
                calib_func += " * Ccoscorr(mua)" # * mu0/mua" (not implemented)
            ds_l1b[var].values = ds_l1b[var].values*C
    
            ds_l1b[var].attrs['units'] = "W m-2",
            ds_l1b[var].attrs.update({
                "units": "W m-2",
                "serial": serial[i],
                "calibration_Cabsolute": Ca,
                "calibration_Ccoscorr": str(np.polynomial.polynomial.Polynomial(CCcoef)),
                "calibration_function": calib_func
            })

    ######################################################################################  
    ## add quality flags
    ds_l1b = pyrnet.qcrad.add_qc_flags(ds_l1b, config["radflux_varname"])

    ######################################################################################
    ## Update variables, global attributes and encoding
    #add global coverage attributes
    ds_l1b = update_coverage_meta(ds_l1b, timevar="time")
    ds_l1b.attrs["processing_level"] = 'l1b'
    ds_l1b.attrs["product_version"] = pyrnet_version
    now = pd.to_datetime(np.datetime64("now"))
    ds_l1b.attrs["history"] = ds_l1b.history + f"{now.isoformat()}: Generated level l1b  by pyrnet version {pyrnet_version}; "

    # update encoding
    ds_l1b = add_encoding(ds_l1b, vencode=vencode)

    return ds_l1b

# %% ../../nbs/pyrnet/data.ipynb 72
def _sort_by_station(dslist):
    # sort dslist for first station
    station0 = []
    for i in range(len(dslist)):
        station0.append(int(dslist[i]["station"].values[0]))
    isort = np.argsort(station0).ravel()
    dslist = [dslist[i] for i in isort]
    return dslist


# %% ../../nbs/pyrnet/data.ipynb 75
def _merge_gattrs_by_station(dslist, merge_gattrs):
    # merge variable attrs:
    merge_gattrs_fill_value = [merge_gattrs[key] for key in merge_gattrs] 
    merge_gattrs = [key for key in merge_gattrs]
    
    merged_gattrs = {}
    gattrs_idx = {}
    for i in range(len(dslist)):
        dst = dslist[i]
        for j,attr in enumerate(merge_gattrs):
            if attr not in dst.attrs:
                fill_value = dst.station.size * [merge_gattrs_fill_value[j]]
                dst.attrs.update({
                    attr: fill_value
                })
            # save station index, as attributes are related to station dimension
            attridx = list(dst.station.values.astype(int))
            attrval = list(pyrnet.utils.make_iter(dst.attrs[attr]))
            # merge attributes, overwrite values of same station
            if attr in merged_gattrs:
                mattrval = merged_gattrs[attr] + attrval
                mattridx = gattrs_idx[attr] + attridx
                _,idx = np.unique(mattridx, return_index=True)
                attridx = [mattridx[i] for i in idx]
                attrval = [mattrval[i] for i in idx]
            gattrs_idx = assoc_in(
                gattrs_idx, [attr], attridx
            )
            merged_gattrs = assoc_in(
                merged_gattrs, [attr], attrval
            )
    return merged_gattrs

def _merge_vattrs_by_station(dslist, merge_attrs):
    # check if gti is in one of the datasets
    gti = False
    for ds in dslist:
        if "gti" in ds:
            gti = True
    
    if gti:
        # add gti variables if some datasets missing gti
        for i in range(len(dslist)):
            dst = dslist[i].copy()
            if "gti" in dst:
                continue
            ghi_vars = [var for var in dst if "ghi" in var]
            gti_vars = [var.replace("ghi","gti") for var in ghi_vars]
            for ghi_var, gti_var in zip(ghi_vars,gti_vars):
                dst = dst.assign({
                    gti_var: (dst[ghi_var].dims, np.full(dst[ghi_var].shape, np.nan))
                })
                for attr in merge_attrs:
                    skip = True
                    for apply_to in merge_attrs[attr]["apply_to"]:
                        if gti_var.startswith(apply_to):
                            skip = False
                    if skip:
                        continue
                    fill_value = dst.station.size * [merge_attrs[attr]["fill_value"]]
                    dst[gti_var].attrs.update({
                        attr: fill_value
                    })
            dslist[i] = dst
    
    merged_attrs = {}
    mattrs_idx = {}
    for i in range(len(dslist)):
        dst = dslist[i]
        for var in dst:
            for attr in merge_attrs:
                skip = True
                for apply_to in merge_attrs[attr]["apply_to"]:
                    if var.startswith(apply_to):
                        skip = False
                if skip:
                    continue
                if attr not in dst[var].attrs:
                    fill_value = dst.station.size * [merge_attrs[attr]["fill_value"]]
                    dst[var].attrs.update({
                        attr: fill_value
                     })
                # save station index, as attributes are related to station dimension
                attridx = list(dst.station.values.astype(int))
                attrval = list(pyrnet.utils.make_iter(dst[var].attrs[attr]))

                # merge attributes, overwrite values of same station
                if var not in merged_attrs:
                    merged_attrs.update({var:{}})
                    mattrs_idx.update({var:{}})
                if attr in merged_attrs[var]:
                    mattrval = merged_attrs[var][attr] + attrval
                    mattridx = mattrs_idx[var][attr] + attridx
                    
                    _,idx = np.unique(mattridx, return_index=True)
                    attridx = [mattridx[i] for i in idx]
                    attrval = [mattrval[i] for i in idx]
                mattrs_idx = assoc_in(
                    mattrs_idx, [var,attr], attridx
                )
                merged_attrs = assoc_in(
                    merged_attrs, [var,attr], attrval
                )
    return dslist, merged_attrs
    

# %% ../../nbs/pyrnet/data.ipynb 78
def _reindex_time(dslist, freq='1s', timevar='time'):
    dates = []
    for i in range(len(dslist)):
        # reindex to full day
        dates.append(dslist[i][timevar].values[0].astype("datetime64[D]"))
    udates = np.unique(dates)
    
    timeidx = pd.DatetimeIndex([])
    for date in udates:
        timeidx = timeidx.append(pd.date_range(
            date,
            date + np.timedelta64(1, 'D'),
            freq=freq,
            inclusive='left'
        ))
    
    for i in range(len(dslist)):
        dslist[i] = dslist[i].reindex(
            {timevar:timeidx},
            method='nearest',
            tolerance=np.timedelta64(1, 'ms')
        )
    return dslist

def _reindex_station(dslist):
    stations = []
    for i in range(len(dslist)):
        # reindex to full day
        stations += list(dslist[i].station.values)
    ustations = np.unique(stations)
    for i in range(len(dslist)):
        dslist[i] = dslist[i].reindex(
            {"station":ustations},
            method='nearest',
            tolerance=1e-6
        )
    return dslist

def _reindex_maintenancetime(dslist):
    mtimes = []
    for i in range(len(dslist)):
        # reindex to full day
        mtimes += list(dslist[i].maintenancetime.values)
    umtimes = np.unique(mtimes)
    for i in range(len(dslist)):
        dslist[i] = dslist[i].reindex(
            {"maintenancetime":umtimes},
            method='nearest',
            tolerance=np.timedelta64(1, 'ms')
        )
    return dslist

# %% ../../nbs/pyrnet/data.ipynb 80
def _maintenancetime_snap_to_gap(ds):
    old_mtimes = ds.maintenancetime.values
    new_mtimes = old_mtimes.copy()
    for i, mtime in enumerate(old_mtimes):
        dtime = mtime.astype("datetime64[D]") - ds.time.values[0].astype("datetime64[D]")
        # consider only same or next day maintenance times for snapping
        if (dtime > np.timedelta64(1,"D") or
            dtime < np.timedelta64(0,"D")):
            continue
        # lookup station
        istation = np.argwhere(~np.isnan(
            ds.maintenance_flag_ghi.values[i,:]
        ))[0][0]
        # identify gaps and gap length
        igap = np.argwhere(np.isnan(
            ds.ghi.isel(station=istation).values
        )).ravel()
        if len(igap)<10:
            continue
        digap = np.diff(igap)
        istartgaps = np.insert(digap,0,0)!=1
        iendgaps = np.insert(digap,-1,0)!=1
        # checkout only gaps of certain length
        gaptimes = []
        gapidxs = []
        for istart,iend in zip(igap[istartgaps],igap[iendgaps]):
            gaptime = ds.time.values[iend] - ds.time.values[istart]
            # assume maintenance takes at least 10 min and maximum 2h
            if (np.timedelta64(10, "m") < gaptime < np.timedelta64(2, 'h')):
                gapidxs.append(istart)
                gaptimes.append(ds.time.values[istart])
        if len(gaptimes)==0:
            continue
        # snap maintenance time to the closest matching gap
        new_mtimes[i] = gaptimes[np.argmin([mtime-gtime for gtime in gaptimes])]
    # update maintenancetime coordinate
    ds = ds.assign_coords({
        "maintenancetime": ("maintenancetime", new_mtimes)
    })
    ds = ds.sortby("maintenancetime")
    
    # merge duplicates
    dates, counts = np.unique(ds.maintenancetime.values,return_counts=True)
    duplets = dates[counts>1]
    if len(duplets)>0:
        dsold = ds.copy()
        ds = ds.drop_duplicates(dim='maintenancetime', keep='first')
        for ddate in duplets:
            dst = dsold.sel(maintenancetime=ddate)
            vars = [var for var in dst if "maintenancetime" in  dst[var].dims]
            for var in vars:
                for i in range(dst.maintenancetime.size-1):
                    mask = np.isnan(ds[var].sel(maintenancetime=ddate).values)
                    ds[var].sel(maintenancetime=ddate).values[mask] = dst[var].isel(maintenancetime=i+1).values[mask]
    
    return ds

# %% ../../nbs/pyrnet/data.ipynb 82
def merge_l1b(
        dslist,
        freq='1s',
        timevar='time',
        merge_gattrs={"site":""},
        merge_attrs={
            "calibration_Cabsolute":dict(
                fill_value=0,
                apply_to=["ghi", "gti"]
            ),
            "serial":dict(
                fill_value="",
                apply_to=["ghi", "gti"]
            ),
            "vangle":dict(
                fill_value=0,
                apply_to=["ghi", "gti"]
            ),
            "hangle":dict(
                fill_value=0,
                apply_to=["ghi", "gti"]
            ),
            "note_general":dict(
                fill_value="",
                apply_to=["maintenance"]
            ),
            "note_clean":dict(
                fill_value="",
                apply_to=["maintenance"]
            ),
            "note_level":dict(
                fill_value="",
                apply_to=["maintenance"]
            ),
        }
):
    logger.info(f"Merging {len(dslist)} datasets.")
    # sort by first station coordinate
    dslist = _sort_by_station(dslist)
    
    ################################################################
    ## Merge Attributes
    # have to merge attributes before reindexing, 
    # as attributes are tied the specific stations in the current datasets
    merged_gattrs = _merge_gattrs_by_station(dslist, merge_gattrs=merge_gattrs)
    dslist, merged_attrs = _merge_vattrs_by_station(dslist, merge_attrs=merge_attrs)
    
    #####################################################################
    ## Unify datasets
    # reindex timevar:
    dslist = _reindex_time(dslist, freq=freq, timevar=timevar)
    # reindex station var:
    dslist = _reindex_station(dslist)
    # reindex maintenancetime var:
    dslist = _reindex_maintenancetime(dslist)
    
    #####################################################################
    ## Merge datasets
    # merge vars with (time,station) dims
    for i in range(len(dslist)):
        dst = dslist[i].copy()
        dst = dst.drop_vars(
            [var for var in dst if not (timevar in dst[var].dims and "station" in dst[var].dims)]
        )
        if i==0:
            ds_time_station = dst.copy()
        else:
            # handle overlapping values by dropping from the first (override from second)
            for var in dst:
                overlap = (~np.isnan(dst[var].values))*(~np.isnan(dst[var].values))
                ds_time_station[var].values = ds_time_station[var].values.astype(float)
                ds_time_station[var].values[overlap] = np.nan
            ds_time_station = ds_time_station.merge(dst)
            
    # merge vars with (maintenancetime, station) dims
    for i in range(len(dslist)):
        dst = dslist[i].copy()
        dst = dst.drop_vars(
            [var for var in dst if not ("maintenancetime" in dst[var].dims and "station" in dst[var].dims)]
        )
        if i==0:
            ds_mtime_station = dst.copy()
        else:
            for var in dst:
                overlap = (~np.isnan(dst[var].values))*(~np.isnan(dst[var].values))
                ds_mtime_station[var].values = ds_mtime_station[var].values.astype(float)
                ds_mtime_station[var].values[overlap] = np.nan
            ds_mtime_station = ds_mtime_station.merge(dst)
    
    # merge vars with (station) dims
    for i in range(len(dslist)):
        dst = dslist[i].copy()
        dst = dst.drop_vars(
            [var for var in dst if not (len(dst[var].dims)==1 and "station" in dst[var].dims)]
        )
        if i==0:
            ds_station = dst.copy()
        else:
            try:
                # works there is no overlap with non null values ( new stations )
                ds_station = ds_station.merge(dst, compat='no_conflicts')
            except:
                # override if station already exists
                # but fill nan values if available in second dataset
                ustations = np.unique(list(ds_station.station.values)+list(dst.station.values))
                ds_station = ds_station.reindex(station=ustations)
                dst = dst.reindex(station=ustations)
                for key in list(set(ds_station.keys())&set(dst.keys())):
                    mask = np.isnan(ds_station[key].values)
                    ds_station[key].values[mask] = dst[key].values[mask]
                ds_station = ds_station.merge(dst, compat='override')
    
    ds_merged = xr.merge([ds_time_station,ds_station,ds_mtime_station])
    
    ###########################################################################
    ## add merged attrs
    # write new history
    now = pd.to_datetime(np.datetime64("now"))
    ds_merged.attrs["history"] = f"{now.isoformat()}: Merged level l1b by pyrnet version {pyrnet_version}; "
    
    # save merged global attrs
    for attr in ds_merged.attrs:
        if attr not in merge_gattrs:
            continue
        ds_merged.attrs[attr] = merged_gattrs[attr]
    
    # save merged variable attrs
    for var in ds_merged:
        for attr in ds_merged[var].attrs:
            if attr not in merge_attrs:
                continue
            ds_merged[var].attrs[attr] = merged_attrs[var][attr]
  
    #############################################################################
    ## maintenance time snap to data gap
    # check maintenance time per station
    # if its in time range, scan for gaps >10min and snap closest maintenance time to this gaps
    ds_merged = _maintenancetime_snap_to_gap(ds_merged)
    
    # update automatic quality flags
    ds_merged = pyrnet.qcrad.add_qc_flags(ds_merged, ["ghi","gti"])
    # add encoding
    ds_merged = add_encoding(ds_merged)
    logger.info("... merging done.")
    return ds_merged

